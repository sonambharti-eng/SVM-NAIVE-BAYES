{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical Questions<"
      ],
      "metadata": {
        "id": "dmn8f2kLO1oF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.1  What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "1ySKxvctPMKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.1 A Support Vector Machine (SVM) is a type of supervised machine learning algorithm primarily used for classification and sometimes for regression tasks. The main goal of SVM is to find the best boundary (or \"hyperplane\") that separates different classes in the feature space.\n",
        "\n",
        "### Here's how it works:\n",
        "\n",
        "1. **Finding the Hyperplane:**\n",
        "   - In a 2D space, this hyperplane is just a straight line, but in higher dimensions, it’s a flat plane or even a more complex structure.\n",
        "   - SVM tries to find the hyperplane that maximizes the margin between two classes. The margin is the distance between the closest data points (called **support vectors**) from each class to the hyperplane.\n",
        "\n",
        "2. **Support Vectors:**\n",
        "   - These are the data points that are closest to the hyperplane. They are the most important data points in defining the hyperplane, hence the name \"Support Vector\" machine.\n",
        "\n",
        "3. **Maximizing the Margin:**\n",
        "   - The goal is to find the hyperplane that maximizes the margin between the classes. A larger margin typically leads to better generalization and a lower chance of overfitting.\n",
        "\n",
        "4. **Kernel Trick (for non-linear separation):**\n",
        "   - When the data is not linearly separable (i.e., it can’t be separated by a straight line or hyperplane), SVM uses a mathematical technique called the **kernel trick**. It transforms the data into a higher-dimensional space where a hyperplane can be used to separate the classes.\n",
        "   - Popular kernel functions include the **linear kernel**, **polynomial kernel**, and **Radial Basis Function (RBF) kernel**.\n",
        "\n",
        "5. **Classification and Regression:**\n",
        "   - **Classification:** In classification problems, SVM assigns data points to categories based on which side of the hyperplane they fall on.\n",
        "   - **Regression (SVR - Support Vector Regression):** SVM can also be used for regression, where the goal is to predict continuous values instead of categories.\n",
        "\n",
        "### Key Strengths of SVM:\n",
        "- Works well for high-dimensional spaces (e.g., text classification).\n",
        "- Effective in cases where the number of dimensions exceeds the number of samples.\n",
        "- Robust to overfitting, especially in high-dimensional spaces.\n",
        "\n",
        "### Weaknesses:\n",
        "- Can be computationally expensive, especially with large datasets.\n",
        "- Choosing the right kernel and setting the proper parameters (like the regularization parameter **C** and kernel parameters) can be tricky.\n",
        "\n",
        "In summary, an SVM is a powerful tool for classification tasks, especially when the data is well-separated or can be made separable in a higher-dimensional space."
      ],
      "metadata": {
        "id": "aP5nv65JPVqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.2 What is the difference between Hard Margin and Soft Margin SVM?"
      ],
      "metadata": {
        "id": "V9--PAipPvac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.2 The main difference between **Hard Margin** and **Soft Margin** SVM lies in how they handle **misclassifications** and **overlapping data**.\n",
        "\n",
        "### 1. **Hard Margin SVM**:\n",
        "   - **Strict Separation**: In a Hard Margin SVM, the algorithm assumes that the data is **perfectly separable**. This means that there is a clear and clean separation between the two classes with no overlap.\n",
        "   - **No Misclassifications Allowed**: There must be a hyperplane that separates all the data points of one class from those of the other class without any errors (i.e., no points are allowed to fall on the wrong side of the hyperplane).\n",
        "   - **Ideal for Linearly Separable Data**: Hard Margin SVM works well when the data is clean and linearly separable. However, it is **not robust** to noise or outliers in the data because any overlap or misclassified data would violate the assumption of perfect separation.\n",
        "   \n",
        "   **Disadvantages**:\n",
        "   - **Not robust to noise**: If there are outliers or some overlap between the classes, the model may fail to find an optimal hyperplane or generalize well.\n",
        "   - **Limited applicability**: It is only suitable for datasets where perfect separation is possible.\n",
        "\n",
        "### 2. **Soft Margin SVM**:\n",
        "   - **Flexible Separation**: Soft Margin SVM allows for some degree of misclassification. It introduces a **penalty** for misclassifications, but does not strictly prohibit them.\n",
        "   - **Slack Variables**: The algorithm introduces **slack variables (ξ)**, which measure how much each data point is allowed to deviate from the correct side of the hyperplane. These slack variables allow some points to be within the margin or on the wrong side of the hyperplane.\n",
        "   - **Trade-off between Margin and Misclassification**: The goal in Soft Margin SVM is to balance the size of the margin and the number of misclassifications by adjusting a parameter called **C**.\n",
        "     - **Large C**: Places more emphasis on minimizing misclassifications, leading to a smaller margin and less flexibility.\n",
        "     - **Small C**: Allows a larger margin and more misclassifications but potentially better generalization.\n",
        "   - **Robust to Noise and Overlapping Data**: Soft Margin SVM is **more robust** to noisy data, outliers, and cases where classes are not perfectly separable.\n",
        "\n",
        "   **Advantages**:\n",
        "   - **Handles non-separable data**: It is well-suited for real-world datasets where perfect separation is not possible.\n",
        "   - **More flexible**: The trade-off parameter (C) allows for fine-tuning of the model's performance based on the data and desired balance between margin width and misclassifications.\n",
        "\n",
        "### Summary of Differences:\n",
        "\n",
        "| **Aspect**              | **Hard Margin SVM**                              | **Soft Margin SVM**                                      |\n",
        "|-------------------------|--------------------------------------------------|----------------------------------------------------------|\n",
        "| **Data Separation**      | Assumes perfect separation (linearly separable) | Allows some misclassifications (non-linearly separable)   |\n",
        "| **Misclassifications**   | No misclassifications allowed                    | Allows misclassifications with a penalty                 |\n",
        "| **Use Case**             | Ideal for perfectly separable data               | Suitable for noisy, overlapping, or non-separable data    |\n",
        "| **Robustness to Noise**  | Not robust to outliers or noise                  | Robust to noise and outliers                             |\n",
        "| **Flexibility**          | Less flexible (strict constraints)               | More flexible (can balance margin and misclassifications) |\n",
        "| **Parameter Control**    | No parameter for flexibility                     | The regularization parameter **C** controls the trade-off |\n",
        "\n",
        "In most real-world scenarios, **Soft Margin SVM** is preferred because it offers flexibility to handle noisy or non-separable data, making it more versatile than **Hard Margin SVM**, which is restrictive and limited to ideal, noise-free datasets."
      ],
      "metadata": {
        "id": "CvBI2d3JQW3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.3 What is the mathematical intuition behind SVM?"
      ],
      "metadata": {
        "id": "CA1AbL2sQqSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.3 The mathematical intuition behind Support Vector Machine (SVM) lies in finding the optimal hyperplane that separates two classes of data points with the largest possible margin. Here’s a breakdown of the key mathematical concepts involved:\n",
        "\n",
        "### 1. **The Hyperplane and Separation:**\n",
        "   - In **n-dimensional space**, a **hyperplane** is a flat, (n-1)-dimensional subspace that divides the space into two halves.\n",
        "   - For binary classification, the goal of SVM is to find a hyperplane that **separates** the two classes of data points.\n",
        "\n",
        "   The equation of a hyperplane can be written as:\n",
        "\n",
        "   \\[\n",
        "   \\mathbf{w}^T \\mathbf{x} + b = 0\n",
        "   \\]\n",
        "\n",
        "   - **w** is the weight vector (normal to the hyperplane).\n",
        "   - **x** is a data point.\n",
        "   - **b** is the bias term (offset from the origin).\n",
        "\n",
        "### 2. **Margin Maximization:**\n",
        "   - The **margin** is the distance between the hyperplane and the closest data points from each class (called **support vectors**).\n",
        "   - The **goal of SVM** is to maximize this margin while maintaining correct classification of the data points.\n",
        "   - If the data points are perfectly separable, SVM tries to maximize the margin between the two classes of points by selecting the optimal hyperplane.\n",
        "\n",
        "   For a given data point, the **margin** is defined as the perpendicular distance from the point to the hyperplane. For any point \\( \\mathbf{x}_i \\), the margin can be written as:\n",
        "\n",
        "   \\[\n",
        "   \\text{Distance} = \\frac{|\\mathbf{w}^T \\mathbf{x}_i + b|}{\\|\\mathbf{w}\\|}\n",
        "   \\]\n",
        "\n",
        "   To **maximize the margin**, we minimize the magnitude of \\( \\mathbf{w} \\), as the margin is inversely proportional to \\( \\|\\mathbf{w}\\| \\). Therefore, the objective is to minimize \\( \\frac{1}{2} \\|\\mathbf{w}\\|^2 \\) (we use \\( \\frac{1}{2} \\) for mathematical convenience, as it simplifies the derivatives later).\n",
        "\n",
        "### 3. **The Constraints for Perfect Classification (Hard Margin SVM):**\n",
        "   For a hyperplane to perfectly separate the classes, all points from one class should lie on one side of the hyperplane, and all points from the other class should lie on the opposite side. Mathematically:\n",
        "\n",
        "   - For points belonging to class +1: \\( \\mathbf{w}^T \\mathbf{x}_i + b \\geq +1 \\)\n",
        "   - For points belonging to class -1: \\( \\mathbf{w}^T \\mathbf{x}_i + b \\leq -1 \\)\n",
        "\n",
        "   These constraints ensure that each data point is correctly classified with a margin of at least 1, i.e., no data point should lie inside or on the wrong side of the margin.\n",
        "\n",
        "   The optimization problem for **Hard Margin SVM** becomes:\n",
        "\n",
        "   \\[\n",
        "   \\text{Minimize:} \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
        "   \\]\n",
        "\n",
        "   **Subject to:**\n",
        "   \\[\n",
        "   y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1, \\quad \\forall i\n",
        "   \\]\n",
        "\n",
        "   where \\( y_i \\) is the label of the \\( i \\)-th data point (either +1 or -1), and \\( \\mathbf{x}_i \\) is the feature vector of the \\( i \\)-th data point.\n",
        "\n",
        "### 4. **Soft Margin SVM (for Non-linearly Separable Data):**\n",
        "   - When the data is not perfectly separable (i.e., some data points might overlap between classes), we introduce **slack variables** \\( \\xi_i \\) to allow some points to be misclassified or fall within the margin.\n",
        "   \n",
        "   The new constraints become:\n",
        "\n",
        "   \\[\n",
        "   y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i, \\quad \\forall i\n",
        "   \\]\n",
        "\n",
        "   where \\( \\xi_i \\geq 0 \\) represents the slack variable for the \\( i \\)-th point, allowing it to be either within the margin or misclassified.\n",
        "\n",
        "   - The optimization problem now becomes:\n",
        "\n",
        "   \\[\n",
        "   \\text{Minimize:} \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
        "   \\]\n",
        "\n",
        "   Here:\n",
        "   - \\( C \\) is a regularization parameter that controls the trade-off between maximizing the margin and minimizing the misclassification error (penalizing the slack variables). A **larger value of C** emphasizes minimizing misclassifications, while a **smaller C** allows more slack (i.e., more misclassifications) but can lead to a larger margin and better generalization.\n",
        "\n",
        "### 5. **Kernel Trick (For Non-linear Separation):**\n",
        "   - In many real-world problems, the data is not linearly separable, meaning you cannot find a hyperplane in the original feature space that perfectly separates the data. To handle this, **SVM uses the kernel trick**.\n",
        "   - A **kernel function** maps the input data from its original space to a higher-dimensional space where a linear separation may be possible. Common kernel functions include:\n",
        "     - **Linear Kernel**: \\( K(\\mathbf{x}, \\mathbf{x}') = \\mathbf{x}^T \\mathbf{x}' \\)\n",
        "     - **Polynomial Kernel**: \\( K(\\mathbf{x}, \\mathbf{x}') = (\\mathbf{x}^T \\mathbf{x}' + 1)^d \\)\n",
        "     - **RBF (Radial Basis Function) Kernel**: \\( K(\\mathbf{x}, \\mathbf{x}') = \\exp(-\\gamma \\|\\mathbf{x} - \\mathbf{x}'\\|^2) \\)\n",
        "\n",
        "   By using kernels, SVM can efficiently find a hyperplane in the transformed feature space without explicitly computing the transformation.\n",
        "\n",
        "### Summary of Mathematical Intuition:\n",
        "- **Objective**: Find a hyperplane that separates data points of two classes while maximizing the margin.\n",
        "- **Optimization**: Minimize the norm of the weight vector \\( \\|\\mathbf{w}\\| \\) subject to constraints that ensure correct classification of points.\n",
        "- **Soft Margin**: Allow misclassifications or margin violations using slack variables and a regularization parameter \\( C \\).\n",
        "- **Kernel Trick**: Use a kernel function to map the data to a higher-dimensional space where linear separation is possible.\n",
        "\n",
        "This approach balances finding the largest margin between classes and controlling misclassifications, with flexibility to handle non-linearly separable data using kernels."
      ],
      "metadata": {
        "id": "BPTAhGWsQzsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.4 What is the role of Lagrange Multipliers in SVM?\n"
      ],
      "metadata": {
        "id": "SPxVYHZ1RK7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.4 Lagrange multipliers play a key role in the **optimization** problem of Support Vector Machines (SVM), specifically in the process of **constrained optimization**. The goal of SVM is to find the optimal hyperplane that maximizes the margin between classes, subject to certain constraints (i.e., ensuring the correct classification of data points). Lagrange multipliers help in formulating and solving this constrained optimization problem.\n",
        "\n",
        "### Why Lagrange Multipliers?\n",
        "\n",
        "In the context of SVM, we want to **maximize the margin** while also ensuring that the data points are correctly classified (i.e., subject to constraints). To do this efficiently, we can use **Lagrange multipliers** to turn the constrained optimization problem into an unconstrained one.\n",
        "\n",
        "### The SVM Optimization Problem (Soft Margin):\n",
        "\n",
        "The objective is to minimize the cost function that represents the margin, while ensuring the constraints are satisfied. For **Soft Margin SVM**, the problem can be written as:\n",
        "\n",
        "\\[\n",
        "\\text{Minimize:} \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i\n",
        "\\]\n",
        "\n",
        "**Subject to the constraints:**\n",
        "\n",
        "\\[\n",
        "y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i \\quad \\forall i, \\quad \\xi_i \\geq 0\n",
        "\\]\n",
        "\n",
        "Here:\n",
        "- \\( \\mathbf{w} \\) is the weight vector.\n",
        "- \\( b \\) is the bias term.\n",
        "- \\( \\xi_i \\) is the slack variable that allows some points to be misclassified.\n",
        "- \\( C \\) is a regularization parameter controlling the trade-off between maximizing the margin and minimizing misclassification.\n",
        "\n",
        "### Using Lagrange Multipliers:\n",
        "\n",
        "To solve this optimization problem, we introduce **Lagrange multipliers** to handle the constraints. The Lagrangian function is constructed by combining the objective function and the constraints using these multipliers.\n",
        "\n",
        "1. **Lagrangian Function:**\n",
        "   We introduce Lagrange multipliers \\( \\alpha_i \\geq 0 \\) for each constraint \\( y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i \\). The Lagrangian is:\n",
        "\n",
        "   \\[\n",
        "   \\mathcal{L}(\\mathbf{w}, b, \\xi, \\alpha) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i - \\sum_{i=1}^n \\alpha_i \\left[ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) - 1 + \\xi_i \\right]\n",
        "   \\]\n",
        "\n",
        "   Here, \\( \\alpha_i \\) are the Lagrange multipliers associated with the constraints.\n",
        "\n",
        "2. **Stationarity Conditions:**\n",
        "   To solve the optimization problem, we differentiate the Lagrangian with respect to \\( \\mathbf{w} \\), \\( b \\), and \\( \\xi_i \\) and set these derivatives to zero to find the **optimal values** of these variables.\n",
        "\n",
        "   - For \\( \\mathbf{w} \\), we have:\n",
        "\n",
        "     \\[\n",
        "     \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{w}} = \\mathbf{w} - \\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i = 0\n",
        "     \\]\n",
        "   \n",
        "     This equation implies that \\( \\mathbf{w} \\) is a **linear combination** of the support vectors, weighted by the Lagrange multipliers \\( \\alpha_i \\).\n",
        "\n",
        "   - For \\( b \\), we differentiate with respect to \\( b \\) and set it to zero:\n",
        "\n",
        "     \\[\n",
        "     \\frac{\\partial \\mathcal{L}}{\\partial b} = \\sum_{i=1}^n \\alpha_i y_i = 0\n",
        "     \\]\n",
        "\n",
        "     This condition ensures that the sum of the Lagrange multipliers associated with the constraints is zero.\n",
        "\n",
        "   - For \\( \\xi_i \\), we differentiate with respect to \\( \\xi_i \\) and set it to zero:\n",
        "\n",
        "     \\[\n",
        "     \\frac{\\partial \\mathcal{L}}{\\partial \\xi_i} = C - \\alpha_i = 0 \\quad \\Rightarrow \\quad \\alpha_i = C\n",
        "     \\]\n",
        "\n",
        "3. **KKT Conditions (Karush-Kuhn-Tucker Conditions):**\n",
        "   The **KKT conditions** are a set of necessary conditions for a solution to be optimal in constrained optimization problems. For SVM, these conditions are:\n",
        "\n",
        "   - **Primal feasibility**: \\( y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i \\)\n",
        "   - **Dual feasibility**: \\( \\alpha_i \\geq 0 \\)\n",
        "   - **Complementary slackness**: \\( \\alpha_i \\left[ y_i (\\mathbf{w}^T \\mathbf{x}_i + b) - 1 + \\xi_i \\right] = 0 \\)\n",
        "     - This means that if \\( \\alpha_i > 0 \\), the constraint is active (i.e., the point is a support vector and lies exactly on the margin), and if \\( \\alpha_i = 0 \\), the constraint is not active (the point is not a support vector).\n",
        "\n",
        "4. **Dual Formulation:**\n",
        "   The Lagrange multipliers \\( \\alpha_i \\) allow us to rewrite the optimization problem in **dual form**. This dual form focuses on maximizing the Lagrangian with respect to \\( \\alpha_i \\) (instead of directly minimizing \\( \\|\\mathbf{w}\\|^2 \\)):\n",
        "\n",
        "   \\[\n",
        "   \\text{Maximize:} \\quad W(\\alpha) = \\sum_{i=1}^n \\alpha_i - \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i^T \\mathbf{x}_j\n",
        "   \\]\n",
        "\n",
        "   **Subject to:**\n",
        "   - \\( \\alpha_i \\geq 0 \\)\n",
        "   - \\( \\sum_{i=1}^n \\alpha_i y_i = 0 \\)\n",
        "\n",
        "   The dual problem is easier to solve, especially when using **kernels** for non-linear SVMs, because it allows us to work with the inner products of the data points rather than directly with their coordinates.\n",
        "\n",
        "### Summary: The Role of Lagrange Multipliers in SVM\n",
        "\n",
        "Lagrange multipliers help convert the **constrained optimization** problem of finding the optimal hyperplane (with constraints) into a more manageable unconstrained problem. They:\n",
        "- Allow us to handle the constraints on the classification of data points.\n",
        "- Provide a way to express the optimal hyperplane \\( \\mathbf{w} \\) as a linear combination of the support vectors.\n",
        "- Lead to the **dual form** of the SVM optimization problem, which is useful for both linear and non-linear SVMs (especially with kernels).\n",
        "- Ensure the **complementary slackness condition**, meaning only the support vectors contribute to the definition of the optimal hyperplane.\n",
        "\n",
        "In essence, Lagrange multipliers are central to the formulation of SVMs, enabling efficient computation and flexibility when working with complex, high-dimensional data."
      ],
      "metadata": {
        "id": "LavaVTb0RWLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.5 What are Support Vectors in SVM?"
      ],
      "metadata": {
        "id": "MajIaZRoRqgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.5 **Support Vectors** in Support Vector Machine (SVM) are the data points that are **critical** in defining the **optimal hyperplane** that separates the classes in the feature space. These are the points that lie closest to the hyperplane, and they are the ones that **support** the margin.\n",
        "\n",
        "### Key Concepts about Support Vectors:\n",
        "\n",
        "1. **Definition**:\n",
        "   - Support vectors are the data points that are closest to the decision boundary (the hyperplane).\n",
        "   - They are the **\"critical\"** data points because they determine the **position** and **orientation** of the optimal hyperplane (the decision boundary).\n",
        "   - These points are on the edges of the margin and are the only ones that directly influence the final model. Other data points that are farther away from the hyperplane do not impact the hyperplane's position, as long as they are correctly classified.\n",
        "\n",
        "2. **Margin**:\n",
        "   - The margin is the distance between the decision boundary (hyperplane) and the closest data points from each class. SVM aims to **maximize** this margin.\n",
        "   - The support vectors are the data points that lie at the **boundary** of this margin.\n",
        "\n",
        "3. **Role in the Optimization Problem**:\n",
        "   - In the SVM optimization process, the objective is to find a hyperplane that maximizes the margin. The margin is defined by the support vectors.\n",
        "   - Mathematically, if you think of the hyperplane as having an equation \\( \\mathbf{w}^T \\mathbf{x} + b = 0 \\), the support vectors are the data points for which \\( y_i (\\mathbf{w}^T \\mathbf{x}_i + b) = 1 \\) for class +1 and \\( y_i (\\mathbf{w}^T \\mathbf{x}_i + b) = -1 \\) for class -1. These points are precisely at the margin boundaries.\n",
        "\n",
        "4. **Importance**:\n",
        "   - Only support vectors affect the position of the hyperplane. **Other points, even if correctly classified, have no effect on the model once the support vectors are identified**.\n",
        "   - Removing a non-support vector from the dataset will not change the decision boundary (unless it becomes a support vector after the removal).\n",
        "   - This is a reason why SVMs are often considered to be **memory efficient**, because they only need to retain the support vectors for model prediction, not the entire dataset.\n",
        "\n",
        "5. **Support Vectors in the Soft Margin Case**:\n",
        "   - In the **Soft Margin SVM**, there may be **misclassifications** or points within the margin due to the introduction of **slack variables** \\( \\xi_i \\). The support vectors in this case could be either:\n",
        "     - **On the margin** (where \\( y_i (\\mathbf{w}^T \\mathbf{x}_i + b) = 1 \\) or \\( y_i (\\mathbf{w}^T \\mathbf{x}_i + b) = -1 \\)).\n",
        "     - **On the wrong side of the margin** (where \\( y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\) is less than 1 but greater than 0, due to slack variables).\n",
        "\n",
        "6. **Support Vectors in the Dual Form**:\n",
        "   - In the **dual formulation** of the SVM problem, the support vectors correspond to those data points for which the corresponding **Lagrange multipliers** \\( \\alpha_i \\) are greater than 0.\n",
        "   - These are the points that contribute to the **optimal hyperplane**. If \\( \\alpha_i = 0 \\), then the point is not a support vector and does not affect the decision boundary.\n",
        "\n",
        "7. **In the Non-Linear Case (with Kernels)**:\n",
        "   - When using **kernels** (such as the RBF kernel) for non-linearly separable data, the support vectors are the data points that are closest to the decision boundary in the **higher-dimensional feature space** created by the kernel. Even though the data may not be separable in the original feature space, the support vectors still play the crucial role in defining the boundary in the transformed space.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "- **Support Vectors** are the data points that are closest to the decision boundary (hyperplane) in SVM.\n",
        "- They are crucial for defining the **margin** and the **optimal hyperplane**.\n",
        "- In SVM, only these support vectors affect the final decision boundary, and removing non-support vectors doesn’t change the model.\n",
        "- Support vectors are determined during the training process and are the key points that SVM uses to make predictions.\n",
        "\n",
        "In essence, support vectors are the \"most important\" data points in SVM, and they are the foundation for the model's ability to generalize effectively."
      ],
      "metadata": {
        "id": "dPgEAgM2R0Bd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.6 What is a Support Vector Classifier (SVC)?"
      ],
      "metadata": {
        "id": "eg08Dsb_TE2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.6 A **Support Vector Classifier (SVC)** is a type of machine learning model that uses the principles of **Support Vector Machines (SVM)** for **binary classification** tasks. It is one of the most commonly used algorithms for classification problems, especially when the goal is to separate data into two categories based on features.\n",
        "\n",
        "### Key Features of Support Vector Classifier (SVC):\n",
        "\n",
        "1. **Classification Task**:\n",
        "   - The primary goal of an SVC is to classify data into one of two classes. Given a set of data points with labeled classes (e.g., +1 and -1), the SVC attempts to find the **optimal decision boundary** (also known as a **hyperplane**) that separates the two classes.\n",
        "\n",
        "2. **Optimal Hyperplane**:\n",
        "   - The SVC seeks to find the hyperplane that **maximizes the margin** between the two classes. The margin is the distance between the hyperplane and the closest data points from either class, known as **support vectors**.\n",
        "   - **Maximizing the margin** is crucial because a larger margin typically results in better generalization to new, unseen data (reducing overfitting).\n",
        "\n",
        "3. **Mathematical Formulation**:\n",
        "   - In **linear SVC**, the decision boundary is a straight line (in 2D) or a hyperplane (in higher dimensions) that can be described by the equation:\n",
        "\n",
        "     \\[\n",
        "     \\mathbf{w}^T \\mathbf{x} + b = 0\n",
        "     \\]\n",
        "\n",
        "     Here, \\( \\mathbf{w} \\) is the weight vector (normal to the hyperplane), and \\( b \\) is the bias term.\n",
        "   - The optimization problem aims to maximize the margin while satisfying the following constraints for the training data:\n",
        "   \n",
        "     \\[\n",
        "     y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 \\quad \\text{for each data point} \\, i\n",
        "     \\]\n",
        "\n",
        "     where \\( y_i \\) is the label of the data point, and \\( \\mathbf{x}_i \\) is the feature vector of the data point.\n",
        "\n",
        "4. **Support Vectors**:\n",
        "   - The data points closest to the hyperplane are called **support vectors**. These points define the margin and are critical in determining the position of the hyperplane. Only support vectors contribute to the final model; other data points do not affect the decision boundary.\n",
        "\n",
        "5. **Linear vs Non-Linear Classification**:\n",
        "   - **Linear SVC**: When the data is **linearly separable**, the SVC algorithm finds a linear decision boundary (a straight line or a hyperplane) that separates the classes.\n",
        "   - **Non-Linear SVC**: When the data is **not linearly separable**, SVC uses a technique called the **kernel trick**. By applying a kernel function, SVC maps the data into a higher-dimensional space where a linear separation becomes possible. Common kernel functions include:\n",
        "     - **Polynomial kernel**\n",
        "     - **Radial Basis Function (RBF) kernel** (also known as Gaussian kernel)\n",
        "     - **Sigmoid kernel**\n",
        "\n",
        "     These kernels allow the SVC to learn complex decision boundaries that cannot be achieved in the original feature space.\n",
        "\n",
        "6. **Soft Margin Classification**:\n",
        "   - In real-world problems, data is rarely perfectly separable. In such cases, **Soft Margin SVC** is used, which allows some misclassifications by introducing **slack variables**. These slack variables represent the degree to which data points violate the margin constraints.\n",
        "   - The regularization parameter \\( C \\) controls the trade-off between maximizing the margin and minimizing the classification errors (misclassifications). A large \\( C \\) emphasizes minimizing misclassifications, while a small \\( C \\) allows for a larger margin but more misclassifications.\n",
        "\n",
        "7. **Support Vector Classifier Objective**:\n",
        "   - The SVC optimization problem involves two key objectives:\n",
        "     1. **Maximizing the margin** between the two classes (the space between the hyperplane and the support vectors).\n",
        "     2. **Minimizing misclassification errors** (for soft margin classification), controlled by the regularization parameter \\( C \\).\n",
        "\n",
        "### How SVC Works:\n",
        "\n",
        "1. **Training**: During training, the SVC algorithm tries to find the optimal hyperplane by solving the following optimization problem:\n",
        "   - **Maximize the margin** between the support vectors.\n",
        "   - **Minimize misclassifications** (in the case of non-linearly separable data) using slack variables and the regularization parameter \\( C \\).\n",
        "\n",
        "2. **Prediction**: Once the model is trained and the optimal hyperplane is determined, SVC makes predictions by checking on which side of the hyperplane a new data point lies.\n",
        "   - If the point lies on the positive side, it is classified as one class (e.g., +1).\n",
        "   - If it lies on the negative side, it is classified as the other class (e.g., -1).\n",
        "\n",
        "### Advantages of SVC:\n",
        "\n",
        "- **Effective in high-dimensional spaces**: SVC performs well when the number of features is greater than the number of samples (high-dimensional data).\n",
        "- **Memory efficient**: It only uses the support vectors for making predictions, which can reduce memory usage.\n",
        "- **Versatile**: By using different kernels, SVC can handle both linear and non-linear classification tasks effectively.\n",
        "- **Robust to overfitting**: With the correct choice of \\( C \\) and the kernel, SVC can generalize well, even for complex datasets.\n",
        "\n",
        "### Disadvantages of SVC:\n",
        "\n",
        "- **Computationally expensive**: Training an SVC can be slow, especially for large datasets, due to the quadratic optimization problem.\n",
        "- **Sensitive to the choice of kernel and hyperparameters**: The performance of SVC heavily depends on the choice of kernel (e.g., linear, RBF) and the tuning of hyperparameters like \\( C \\) and the kernel parameters (e.g., gamma in the RBF kernel).\n",
        "- **Not suitable for very large datasets**: The algorithm can become inefficient with very large datasets in terms of both memory and computation time.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "A **Support Vector Classifier (SVC)** is a powerful classification model that uses the concept of a **hyperplane** to separate data into different classes. It works by finding the optimal hyperplane that maximizes the margin between the classes while minimizing misclassifications (in case of non-linearly separable data). The SVC can be extended to handle non-linear problems by using **kernels**, and it is particularly useful in high-dimensional spaces."
      ],
      "metadata": {
        "id": "XjYci9aPTO6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.7 What is a Support Vector Regressor (SVR)?"
      ],
      "metadata": {
        "id": "z1LwvzEzTmF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.7 A **Support Vector Regressor (SVR)** is a type of machine learning algorithm used for **regression tasks**, i.e., predicting continuous values, rather than classifying data into categories. Like the Support Vector Machine (SVM) for classification, SVR uses the principles of SVM, but instead of trying to separate classes with a hyperplane, it tries to find a function that best fits the data within a certain margin of tolerance.\n",
        "\n",
        "### Key Features of Support Vector Regressor (SVR):\n",
        "\n",
        "1. **Goal**:\n",
        "   - The primary goal of SVR is to **fit a function** that predicts continuous output values (as opposed to discrete labels in classification).\n",
        "   - SVR tries to find a function that has at most **ε (epsilon)** deviation from the actual target values for the training data points, while still being as **flat** as possible (i.e., with the smallest possible weights).\n",
        "\n",
        "2. **The ε-Insensitive Loss Function**:\n",
        "   - In SVR, we do not penalize predictions that are within a margin of **ε** from the actual target values. This means that for data points that lie within this margin (also called the **ε-tube** or **ε-margin**), no penalty is applied.\n",
        "   - This helps in making the model robust to small fluctuations in the data, reducing overfitting.\n",
        "   - For points outside this margin, we penalize the deviation from the target value.\n",
        "\n",
        "   The loss function used in SVR is a variation of the **epsilon-insensitive loss function**:\n",
        "\n",
        "   \\[\n",
        "   L(\\epsilon) = \\begin{cases}\n",
        "   0 & \\text{if } |y_i - f(\\mathbf{x}_i)| \\leq \\epsilon \\\\\n",
        "   |y_i - f(\\mathbf{x}_i)| - \\epsilon & \\text{otherwise}\n",
        "   \\end{cases}\n",
        "   \\]\n",
        "\n",
        "   Where:\n",
        "   - \\( y_i \\) is the actual target value.\n",
        "   - \\( f(\\mathbf{x}_i) \\) is the predicted value for the input feature vector \\( \\mathbf{x}_i \\).\n",
        "   - \\( \\epsilon \\) is the margin within which no penalty is given.\n",
        "\n",
        "3. **Optimization Problem**:\n",
        "   - Similar to SVM, the SVR problem is a **constrained optimization** problem.\n",
        "   - The goal is to find a function \\( f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b \\) (a hyperplane in the feature space) that minimizes the deviations (errors) outside the \\( \\epsilon \\)-tube while also keeping the model as simple as possible (minimizing the weight vector \\( \\mathbf{w} \\)).\n",
        "   - The optimization problem in SVR can be formulated as:\n",
        "\n",
        "     \\[\n",
        "     \\min_{\\mathbf{w}, b, \\xi_i, \\xi_i^*} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n (\\xi_i + \\xi_i^*)\n",
        "     \\]\n",
        "\n",
        "     **Subject to:**\n",
        "\n",
        "     \\[\n",
        "     |y_i - \\mathbf{w}^T \\mathbf{x}_i - b| \\leq \\epsilon + \\xi_i\n",
        "     \\]\n",
        "     \\[\n",
        "     \\xi_i \\geq 0, \\quad \\xi_i^* \\geq 0\n",
        "     \\]\n",
        "\n",
        "     Where:\n",
        "     - \\( \\mathbf{w} \\) and \\( b \\) are the parameters defining the hyperplane.\n",
        "     - \\( \\xi_i \\) and \\( \\xi_i^* \\) are slack variables that allow for deviations from the \\( \\epsilon \\)-tube.\n",
        "     - \\( C \\) is the regularization parameter that controls the trade-off between fitting the training data well and keeping the model's complexity low.\n",
        "\n",
        "4. **Regularization**:\n",
        "   - The **regularization parameter \\( C \\)** controls the trade-off between allowing deviations from the margin (slack variables) and keeping the model as simple (flat) as possible.\n",
        "   - A large \\( C \\) means the model tries hard to fit the training data without deviations, potentially leading to overfitting.\n",
        "   - A small \\( C \\) allows more flexibility, letting the model tolerate more deviation but potentially underfitting the data.\n",
        "\n",
        "5. **Kernels in SVR**:\n",
        "   - Just like SVM, SVR can also use **kernels** to handle non-linear relationships in the data. This is especially useful when the data cannot be separated by a simple linear function.\n",
        "   - Common kernels include:\n",
        "     - **Linear kernel** (used when the data is linearly separable).\n",
        "     - **Polynomial kernel**.\n",
        "     - **Radial Basis Function (RBF) kernel** (often used for more complex, non-linear data).\n",
        "\n",
        "   The kernel trick allows SVR to map input data into a higher-dimensional space where a linear regression model can be applied, even if the data is not linearly separable in the original space.\n",
        "\n",
        "6. **Prediction**:\n",
        "   - Once the SVR model is trained, it predicts new values by evaluating the function \\( f(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b \\), similar to a linear regression model.\n",
        "   - However, it may involve mapping the data into a higher-dimensional feature space (using the kernel trick) before making the prediction.\n",
        "\n",
        "### Advantages of SVR:\n",
        "\n",
        "1. **Robust to Outliers**: The **ε-insensitive loss** function makes SVR robust to small outliers in the data, as they are ignored as long as they lie within the epsilon margin.\n",
        "2. **Flexible and Powerful**: SVR can handle both **linear** and **non-linear** regression problems by using different kernels.\n",
        "3. **Good Generalization**: By controlling the margin and using the regularization parameter \\( C \\), SVR tends to generalize well to unseen data, avoiding overfitting or underfitting.\n",
        "\n",
        "### Disadvantages of SVR:\n",
        "\n",
        "1. **Computationally Expensive**: SVR can be computationally expensive, especially with large datasets, as it requires solving a quadratic optimization problem.\n",
        "2. **Sensitive to Hyperparameters**: The performance of the SVR is highly sensitive to the choice of hyperparameters \\( C \\), \\( \\epsilon \\), and the kernel parameters (e.g., gamma for RBF kernel). Tuning these hyperparameters requires careful cross-validation.\n",
        "3. **Limited Interpretability**: SVR models, especially with kernels, can be hard to interpret, as they involve high-dimensional feature spaces or complex transformations.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "- **Support Vector Regressor (SVR)** is a machine learning algorithm used for regression tasks that finds a function to predict continuous values.\n",
        "- SVR uses the concept of a **hyperplane** to fit the data, with the goal of maximizing the margin while allowing small deviations within a specified threshold (\\( \\epsilon \\)-insensitive loss).\n",
        "- It can be applied to both **linear** and **non-linear** regression problems using kernels.\n",
        "- The regularization parameter \\( C \\) and the epsilon margin \\( \\epsilon \\) control the trade-off between fitting the data well and keeping the model as simple as possible.\n",
        "\n",
        "SVR is widely used for applications in time series prediction, financial forecasting, and other regression problems where robustness and high accuracy are important."
      ],
      "metadata": {
        "id": "FuJv2CqaT9n5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.8 What is the Kernel Trick in SVM?\n"
      ],
      "metadata": {
        "id": "rz7UCobFUIiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.8 The **Kernel Trick** in Support Vector Machines (SVM) is a powerful technique used to transform data into a higher-dimensional feature space without actually computing the coordinates of the data points in that space. It allows SVMs to find a linear hyperplane in a higher-dimensional space, even when the data is **non-linearly separable** in its original feature space.\n",
        "\n",
        "### Why Use the Kernel Trick?\n",
        "\n",
        "In many real-world problems, the data cannot be separated by a simple linear decision boundary in the original feature space. However, the **Kernel Trick** allows us to implicitly map the data into a higher-dimensional space, where the data might become linearly separable. Once the data is transformed into this higher-dimensional space, SVM can find a **linear hyperplane** that separates the classes effectively.\n",
        "\n",
        "### The Key Idea:\n",
        "\n",
        "The main idea behind the **Kernel Trick** is to compute the **dot product** of the data points in the higher-dimensional feature space **without explicitly performing the mapping**. This avoids the computational cost of directly working in the higher-dimensional space.\n",
        "\n",
        "Instead of computing the coordinates of data points in the higher-dimensional space, we use a **kernel function** to compute the **dot product** in the transformed space. The kernel function computes this dot product directly in the higher-dimensional space based on the original data points in the lower-dimensional space.\n",
        "\n",
        "### Mathematical Explanation:\n",
        "\n",
        "In an SVM, the decision function is based on the dot product between data points:\n",
        "\n",
        "\\[\n",
        "f(\\mathbf{x}) = \\sum_{i=1}^{N} \\alpha_i y_i \\mathbf{x}_i^T \\mathbf{x} + b\n",
        "\\]\n",
        "\n",
        "Where \\( \\mathbf{x}_i \\) are the training data points, \\( \\alpha_i \\) are the Lagrange multipliers, and \\( y_i \\) are the labels of the data points.\n",
        "\n",
        "In the case of a non-linear decision boundary, the kernel trick allows us to compute this dot product in a higher-dimensional space without explicitly mapping the data points. Instead of \\( \\mathbf{x}_i^T \\mathbf{x} \\), we compute a **kernel function** \\( K(\\mathbf{x}_i, \\mathbf{x}_j) \\), which represents the dot product of the data points in the transformed space.\n",
        "\n",
        "### Common Kernel Functions:\n",
        "\n",
        "1. **Linear Kernel** (No mapping, used for linearly separable data):\n",
        "   \\[\n",
        "   K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j\n",
        "   \\]\n",
        "   - This is just the standard dot product, and it doesn't actually transform the data.\n",
        "\n",
        "2. **Polynomial Kernel** (Maps the data to a higher-dimensional space using polynomial functions):\n",
        "   \\[\n",
        "   K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + c)^d\n",
        "   \\]\n",
        "   - Where \\( c \\) is a constant and \\( d \\) is the degree of the polynomial.\n",
        "   - This kernel maps the data into a higher-dimensional space based on polynomial functions, which can help in separating non-linearly separable data.\n",
        "\n",
        "3. **Radial Basis Function (RBF) Kernel** (Also known as the Gaussian kernel, it maps the data into an infinite-dimensional space):\n",
        "   \\[\n",
        "   K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)\n",
        "   \\]\n",
        "   - This kernel is particularly popular and is very powerful for handling non-linear decision boundaries.\n",
        "   - The RBF kernel computes the similarity between points based on the distance between them. Points that are close together in the feature space will have higher similarity.\n",
        "\n",
        "4. **Sigmoid Kernel** (Similar to the activation function used in neural networks):\n",
        "   \\[\n",
        "   K(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\mathbf{x}_i^T \\mathbf{x}_j + c)\n",
        "   \\]\n",
        "   - This kernel is less commonly used but can be effective in certain cases, especially for data that resembles a neural network-like structure.\n",
        "\n",
        "### How the Kernel Trick Works:\n",
        "\n",
        "1. **Implicit Mapping**: Instead of explicitly mapping the data to a higher-dimensional space, the kernel trick computes the **dot product** in the higher-dimensional space directly. This is much more efficient than computing the mapping explicitly.\n",
        "\n",
        "2. **Computational Efficiency**: The kernel function allows SVM to work in high-dimensional spaces without actually transforming the data points into those dimensions. This is crucial, as directly mapping data to higher dimensions could be computationally expensive or even infeasible for very high-dimensional spaces.\n",
        "\n",
        "3. **Flexibility**: By using different kernel functions, SVMs can handle a wide range of problems:\n",
        "   - **Linear separability**: The linear kernel is used when the data is already linearly separable.\n",
        "   - **Non-linear separability**: Non-linear kernels like the polynomial or RBF kernels are used when the data is not linearly separable in the original feature space.\n",
        "\n",
        "### Example: Non-linear SVM using RBF Kernel\n",
        "\n",
        "Consider the case where you have a set of points that are **not linearly separable** in the original 2D space, like two interlocking crescent shapes (a \"moon\" shape). A linear SVM would not be able to find a separating hyperplane for these points. However, by using an **RBF kernel**, SVM can implicitly map these points into a higher-dimensional space where they become linearly separable. After applying the kernel trick, the SVM can find a decision boundary that separates the two classes effectively.\n",
        "\n",
        "### Summary of the Kernel Trick:\n",
        "\n",
        "- **The Kernel Trick** allows SVM to operate in higher-dimensional spaces without explicitly transforming the data, which makes it computationally efficient.\n",
        "- **Kernel functions** (such as linear, polynomial, and RBF) compute the dot product in the higher-dimensional space directly, allowing SVM to learn complex non-linear decision boundaries.\n",
        "- The kernel trick is **essential** for handling non-linear data and is one of the most powerful features of SVM.\n",
        "- By using kernels, SVM can learn complex decision boundaries that might be impossible to separate linearly in the original feature space."
      ],
      "metadata": {
        "id": "uI93kvPiUgSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.9 Compare Linear Kernel, Polynomial Kernel, and RBF Kernel:"
      ],
      "metadata": {
        "id": "o2hxVnDYUx1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.9 The **Linear Kernel**, **Polynomial Kernel**, and **Radial Basis Function (RBF) Kernel** are all different types of **kernel functions** used in Support Vector Machines (SVM) to handle non-linearly separable data by implicitly mapping it into higher-dimensional spaces. Here’s a detailed comparison of these three kernel types:\n",
        "\n",
        "### 1. **Linear Kernel**\n",
        "\n",
        "#### Definition:\n",
        "The **linear kernel** is the simplest type of kernel, and it does not map the data into a higher-dimensional space. It simply computes the dot product of two data points in the original feature space.\n",
        "\n",
        "\\[\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j\n",
        "\\]\n",
        "\n",
        "#### When to Use:\n",
        "- **Linearly separable data**: The linear kernel is effective when the data can be separated by a straight line or hyperplane in the original feature space.\n",
        "- **Simple and fast**: It is computationally efficient, especially when the data is already linearly separable or approximately linearly separable.\n",
        "\n",
        "#### Pros:\n",
        "- **Simplicity**: The linear kernel is straightforward and easy to understand.\n",
        "- **Computationally efficient**: It requires less memory and is faster to compute than more complex kernels.\n",
        "- **No transformation**: The data is not mapped into a higher-dimensional space, so no additional overhead is involved.\n",
        "\n",
        "#### Cons:\n",
        "- **Limited flexibility**: It can only model linear relationships between the data. If the data is not linearly separable, a linear kernel won't work well.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Polynomial Kernel**\n",
        "\n",
        "#### Definition:\n",
        "The **polynomial kernel** computes the dot product of two data points raised to a certain power, effectively transforming the data into a higher-dimensional space. The kernel is defined as:\n",
        "\n",
        "\\[\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + c)^d\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\(c\\) is a constant (a bias term),\n",
        "- \\(d\\) is the degree of the polynomial.\n",
        "\n",
        "#### When to Use:\n",
        "- **Non-linear relationships**: The polynomial kernel is useful when the data is not linearly separable but still exhibits some form of polynomial-like non-linearity.\n",
        "- **Higher-dimensional separability**: When the decision boundary between classes can be represented by a polynomial curve or surface in a higher-dimensional space.\n",
        "\n",
        "#### Pros:\n",
        "- **Flexibility**: It can model more complex relationships than the linear kernel by capturing polynomial patterns in the data.\n",
        "- **Higher dimensionality**: The polynomial kernel transforms the data into a higher-dimensional space, enabling SVM to separate non-linear data.\n",
        "\n",
        "#### Cons:\n",
        "- **Computational cost**: The higher the degree \\(d\\), the more computationally expensive the kernel becomes.\n",
        "- **Risk of overfitting**: A high degree polynomial can lead to overfitting, as the model may become overly complex and fit noise in the data.\n",
        "- **Sensitive to \\(c\\) and \\(d\\)**: The choice of the constant \\(c\\) and the degree \\(d\\) can significantly impact model performance, requiring careful tuning.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Radial Basis Function (RBF) Kernel**\n",
        "\n",
        "#### Definition:\n",
        "The **RBF kernel**, also known as the **Gaussian kernel**, computes the similarity between two points based on their Euclidean distance. It transforms the data into an infinite-dimensional feature space. The RBF kernel is defined as:\n",
        "\n",
        "\\[\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2 \\) is the squared Euclidean distance between the data points \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\),\n",
        "- \\( \\sigma \\) (or sometimes \\( \\gamma \\)) is a parameter that defines the spread of the kernel.\n",
        "\n",
        "#### When to Use:\n",
        "- **Non-linear separability**: The RBF kernel is widely used when data is highly non-linearly separable, and there are complex patterns between the classes.\n",
        "- **Complex decision boundaries**: It is effective for problems where the decision boundary between classes is highly irregular or curved.\n",
        "\n",
        "#### Pros:\n",
        "- **Flexibility and power**: The RBF kernel can handle very complex, non-linear relationships in the data, making it suitable for a wide variety of problems.\n",
        "- **Infinite-dimensional space**: It maps the data into an infinite-dimensional space, which allows for complex decision boundaries.\n",
        "- **Widely used**: It is the most commonly used kernel due to its ability to handle a wide range of non-linear problems.\n",
        "\n",
        "#### Cons:\n",
        "- **Computationally expensive**: The RBF kernel can be computationally expensive, especially when there are many data points and/or high-dimensional data.\n",
        "- **Sensitive to \\( \\sigma \\) or \\( \\gamma \\)**: The kernel's performance is highly sensitive to the choice of the \\( \\sigma \\) (or \\( \\gamma \\)) parameter. If this parameter is not tuned properly, it can lead to underfitting (when \\( \\sigma \\) is too large) or overfitting (when \\( \\sigma \\) is too small).\n",
        "- **Difficult to interpret**: The resulting decision boundaries can be hard to interpret, especially in higher-dimensional spaces.\n",
        "\n",
        "---\n",
        "\n",
        "### Comparison Summary:\n",
        "\n",
        "| **Kernel Type**       | **Equation**                                        | **When to Use**                          | **Pros**                                                      | **Cons**                                                      |\n",
        "|-----------------------|-----------------------------------------------------|------------------------------------------|--------------------------------------------------------------|--------------------------------------------------------------|\n",
        "| **Linear Kernel**      | \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j \\) | Linearly separable data                  | Simple, fast, and efficient for linearly separable data        | Only works for linear relationships                           |\n",
        "| **Polynomial Kernel**  | \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T \\mathbf{x}_j + c)^d \\) | Data with polynomial relationships        | Flexible and can handle polynomial relationships                | Can be computationally expensive, prone to overfitting with high degree \\(d\\) |\n",
        "| **RBF Kernel**         | \\( K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right) \\) | Highly non-linearly separable data       | Very powerful and flexible, works well in high-dimensional spaces | Computationally expensive, sensitive to the choice of \\( \\gamma \\) |\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "- **Linear Kernel** is best for simple, linearly separable data where no transformation is needed.\n",
        "- **Polynomial Kernel** is useful for data that follows polynomial relationships but can be computationally expensive and sensitive to parameters.\n",
        "- **RBF Kernel** is the most flexible and widely used kernel for complex, non-linearly separable data, but it can be computationally expensive and requires careful tuning of hyperparameters like \\( \\gamma \\).\n",
        "\n",
        "In practice, the choice of kernel depends on the nature of the data and the problem at hand, and it's often beneficial to experiment with different kernels and tune the parameters to achieve the best results."
      ],
      "metadata": {
        "id": "C6espSowdoxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.10 What is the effect of the C parameter in SVM?"
      ],
      "metadata": {
        "id": "3iGWiG16d7rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.10 The **C parameter** in Support Vector Machines (SVM) plays a crucial role in controlling the **trade-off between achieving a low error on the training data** and ensuring that the model **generalizes well** to unseen data. It is a regularization parameter that influences the **complexity** of the model and the **margin size** in SVM.\n",
        "\n",
        "### Mathematical Role of C:\n",
        "\n",
        "The C parameter appears in the SVM optimization problem and is part of the objective function. When solving the SVM optimization problem, the goal is to maximize the margin between the classes while minimizing classification errors. The objective function in SVM is formulated as:\n",
        "\n",
        "\\[\n",
        "\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\mathbf{w} \\) and \\( b \\) are the parameters defining the decision hyperplane.\n",
        "- \\( \\xi_i \\) are the slack variables, which represent the degree to which each data point violates the margin (i.e., the points that are misclassified or lie inside the margin).\n",
        "- \\( C \\) is the regularization parameter that controls the penalty on these violations.\n",
        "\n",
        "### Effect of C:\n",
        "\n",
        "1. **Large C (high penalty for misclassification)**:\n",
        "   - A **large value of C** forces the model to minimize the **training error** as much as possible, meaning that the model will try to fit the training data as perfectly as possible.\n",
        "   - The model will place less emphasis on the margin and more on minimizing the slack variables \\( \\xi_i \\). This means that the model will allow fewer misclassifications, even if it results in a **narrower margin**.\n",
        "   - **Risk of overfitting**: When C is too large, the SVM may overfit the training data, especially if there is noise or outliers. The model might perfectly separate the training data but perform poorly on unseen data (low generalization).\n",
        "   \n",
        "   In other words, a large \\( C \\) leads to a **complex model** that is highly influenced by individual data points and minimizes errors at the cost of flexibility.\n",
        "\n",
        "2. **Small C (low penalty for misclassification)**:\n",
        "   - A **small value of C** allows the model to tolerate more misclassifications (i.e., larger slack variables), focusing more on maximizing the **margin** rather than minimizing training errors.\n",
        "   - The model will be more **flexible**, allowing for a **wider margin**, and will not be overly influenced by outliers or noise in the training data.\n",
        "   - **Risk of underfitting**: If C is too small, the model may not fit the training data well and could underfit. It may fail to capture important patterns in the data, leading to poor performance on both the training set and the test set.\n",
        "   \n",
        "   A small \\( C \\) results in a **simpler model** that may not perfectly classify the training data but could generalize better to unseen data.\n",
        "\n",
        "### Visualizing the Effect of C:\n",
        "- **High C (Large penalty for errors)**: The SVM will create a **smaller margin** with fewer misclassifications. However, this can lead to overfitting if there is noise in the data.\n",
        "- **Low C (Small penalty for errors)**: The SVM will create a **larger margin**, allowing more misclassifications but potentially leading to better generalization to new data.\n",
        "\n",
        "### Practical Considerations:\n",
        "- **Tuning C**: The value of C must be tuned carefully. If \\( C \\) is too large, you risk overfitting, but if it's too small, you risk underfitting. Typically, \\( C \\) is tuned using **cross-validation** or a **grid search** over a range of values to find the optimal balance between bias and variance.\n",
        "  \n",
        "- **Relation to Overfitting and Underfitting**:\n",
        "  - A **larger C** leads to **lower bias** but **higher variance**, meaning the model is more likely to memorize the training data (overfit).\n",
        "  - A **smaller C** leads to **higher bias** but **lower variance**, meaning the model might generalize better but fail to capture the complexities of the training data.\n",
        "\n",
        "### Summary of the Effect of C:\n",
        "\n",
        "| **C Value**         | **Bias** | **Variance** | **Model Behavior**                                              |\n",
        "|---------------------|----------|--------------|------------------------------------------------------------------|\n",
        "| **Large C**         | Low      | High         | The model tries to fit the training data perfectly (low bias) but may overfit and generalize poorly (high variance). |\n",
        "| **Small C**         | High     | Low          | The model is more flexible and allows for some misclassification (higher bias), but it may generalize better to unseen data (lower variance). |\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The **C parameter** in SVM controls the trade-off between **fitting the training data** and **maximizing the margin** for better generalization. A larger C focuses on minimizing training errors (leading to a more complex model), while a smaller C allows for more misclassifications but could result in a simpler, more generalizable model. Proper tuning of \\( C \\) is essential to achieve the right balance between underfitting and overfitting."
      ],
      "metadata": {
        "id": "8Hq79wSNeDYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.11 What is the role of the Gamma parameter in RBF Kernel SVM?"
      ],
      "metadata": {
        "id": "woM-PTlGeQKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.11 The **Gamma (γ) parameter** in the **Radial Basis Function (RBF) kernel** for Support Vector Machines (SVM) plays a critical role in defining the **shape and flexibility** of the decision boundary, as it controls the **influence** of each data point in the decision-making process.\n",
        "\n",
        "In the context of the RBF kernel, gamma is used to calculate the similarity between data points, and its value directly impacts how much influence a single training point has on the decision boundary.\n",
        "\n",
        "### Mathematical Definition of Gamma in RBF Kernel\n",
        "\n",
        "The RBF kernel computes the similarity between two data points \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\) using the squared Euclidean distance between them:\n",
        "\n",
        "\\[\n",
        "K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2 \\gamma^2}\\right)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2 \\) is the squared Euclidean distance between the two data points \\( \\mathbf{x}_i \\) and \\( \\mathbf{x}_j \\),\n",
        "- \\( \\gamma \\) (Gamma) is the parameter that controls the **width** of the Gaussian function.\n",
        "\n",
        "### Role of Gamma (γ):\n",
        "\n",
        "The value of **gamma** determines the **extent of the influence** of each data point on the decision boundary. More specifically, gamma controls how far the influence of a single training point reaches:\n",
        "\n",
        "1. **Large Gamma (High Value of γ)**:\n",
        "   - **Effect on the Model**: When \\( \\gamma \\) is large, the kernel function becomes more sensitive to the distance between points. In this case, the influence of each data point is **localized** and only affects nearby points. Essentially, the decision boundary will be **highly sensitive** to the specific points and can result in **complex and tightly fitting decision boundaries**.\n",
        "   - **Risk of Overfitting**: A very large value of gamma can lead to a model that **overfits** the training data. This is because the model will try to fit the decision boundary very closely to the data points, leading to a complex boundary that might not generalize well to unseen data.\n",
        "   \n",
        "   - **Behavior**: The model becomes more likely to classify points very specifically, even if they are outliers or noise, which reduces the ability to generalize.\n",
        "\n",
        "2. **Small Gamma (Low Value of γ)**:\n",
        "   - **Effect on the Model**: When \\( \\gamma \\) is small, the kernel function's influence is more **spread out**, meaning that a larger region of the input space is affected by each training point. This results in a **smoother decision boundary**, as the model is less sensitive to individual data points.\n",
        "   - **Risk of Underfitting**: A very small value of gamma can lead to **underfitting**. This happens because the decision boundary becomes too simple and fails to capture the underlying patterns in the data. The model might be too general, and as a result, it may not perform well on the training set or the test set.\n",
        "\n",
        "   - **Behavior**: The model may not differentiate enough between classes and fail to capture the complexity of the data.\n",
        "\n",
        "### Visualizing the Effect of Gamma:\n",
        "\n",
        "- **High Gamma**: A high value of gamma causes the decision boundary to be **very wavy and close** to the training data points. The decision boundary may zigzag around the data points, resulting in an overly complex model that fits the training data well but does not generalize to new data.\n",
        "- **Low Gamma**: A low value of gamma causes the decision boundary to be **smooth and less complex**, leading to a simpler, more generalizable model but possibly failing to capture the complexities of the training data.\n",
        "\n",
        "### Example:\n",
        "\n",
        "1. **Large Gamma (e.g., \\( \\gamma = 10 \\))**:\n",
        "   - The decision boundary will be **tight around the data points**, and the SVM will focus heavily on correctly classifying individual points, especially those that are close to the margin.\n",
        "   - This can result in **overfitting**, where the model captures too much noise and detail from the training data.\n",
        "\n",
        "2. **Small Gamma (e.g., \\( \\gamma = 0.1 \\))**:\n",
        "   - The decision boundary will be **broader and smoother**, and the SVM will focus more on the general structure of the data rather than the exact locations of individual points.\n",
        "   - This can result in **underfitting**, where the model is too simple and may fail to accurately classify points, especially if the data has complex relationships.\n",
        "\n",
        "### Gamma and the Trade-Off Between Bias and Variance:\n",
        "\n",
        "- **Large Gamma**:\n",
        "   - **Bias**: Low bias (model fits the training data well).\n",
        "   - **Variance**: High variance (model is sensitive to small changes in the data and overfits).\n",
        "   \n",
        "- **Small Gamma**:\n",
        "   - **Bias**: High bias (model fails to capture the complexity of the data).\n",
        "   - **Variance**: Low variance (model is simpler and generalizes better, but may underfit).\n",
        "\n",
        "### Tuning Gamma:\n",
        "\n",
        "- The optimal value of gamma depends on the **data** and the **specific problem** you are solving. You can use techniques like **cross-validation** or **grid search** to find the best value of gamma that balances bias and variance.\n",
        "- The value of gamma is often chosen in conjunction with the **C parameter** (regularization parameter) to achieve a good trade-off between fitting the training data and generalizing to unseen data.\n",
        "\n",
        "### Summary of Gamma’s Role:\n",
        "\n",
        "| **Gamma Value**     | **Model Behavior**                                  | **Risk**                                  | **Bias** | **Variance** |\n",
        "|---------------------|-----------------------------------------------------|-------------------------------------------|----------|--------------|\n",
        "| **Large Gamma**     | Small influence of each data point, complex decision boundary, fits training data tightly. | **Overfitting**: Model is too sensitive to noise and specific data points. | Low      | High         |\n",
        "| **Small Gamma**     | Large influence of each data point, smooth decision boundary, simpler model. | **Underfitting**: Model is too simple, may not capture data patterns. | High     | Low          |\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "- **Gamma** controls how much influence a single data point has on the decision boundary. A large gamma leads to a **tight, complex decision boundary** that may overfit the data, while a small gamma leads to a **smoother decision boundary** that may underfit the data.\n",
        "- Proper **tuning of gamma** is crucial to balance the trade-off between bias and variance and to ensure that the SVM model generalizes well to unseen data."
      ],
      "metadata": {
        "id": "6YKRQz27ej-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.12 What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n"
      ],
      "metadata": {
        "id": "5Jw0ywQXeo1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.12 The **Naïve Bayes classifier** is a **probabilistic machine learning algorithm** based on **Bayes' Theorem**, used for classification tasks. It is particularly known for its simplicity and efficiency, especially when dealing with large datasets.\n",
        "\n",
        "The classifier works by calculating the **probability** of different classes based on the features of the data and then assigns the class with the highest probability to a new data point. It assumes that the features used for classification are **conditionally independent** given the class label, which is where the \"naïve\" part comes from.\n",
        "\n",
        "### Bayes' Theorem:\n",
        "\n",
        "Bayes' Theorem describes the relationship between conditional probabilities. It is given by:\n",
        "\n",
        "\\[\n",
        "P(C_k | X) = \\frac{P(X | C_k) P(C_k)}{P(X)}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\(P(C_k | X)\\) is the **posterior probability** of class \\(C_k\\) given the features \\(X\\).\n",
        "- \\(P(X | C_k)\\) is the **likelihood** of observing the features \\(X\\) given class \\(C_k\\).\n",
        "- \\(P(C_k)\\) is the **prior probability** of class \\(C_k\\).\n",
        "- \\(P(X)\\) is the **evidence** or the total probability of the features \\(X\\), which normalizes the result.\n",
        "\n",
        "### How the Naïve Bayes Classifier Works:\n",
        "\n",
        "1. **Assume independence**: The key assumption in Naïve Bayes is that the features are **conditionally independent** given the class label. This means that the probability of observing the data features can be written as the product of the individual probabilities for each feature:\n",
        "\n",
        "\\[\n",
        "P(X | C_k) = P(x_1 | C_k) P(x_2 | C_k) \\dots P(x_n | C_k)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\(x_1, x_2, ..., x_n\\) are the individual features of the data point.\n",
        "\n",
        "2. **Calculate class probabilities**: Using Bayes' Theorem and the independence assumption, the Naïve Bayes classifier computes the posterior probability for each class:\n",
        "\n",
        "\\[\n",
        "P(C_k | X) \\propto P(C_k) \\prod_{i=1}^{n} P(x_i | C_k)\n",
        "\\]\n",
        "\n",
        "3. **Classify**: The algorithm then selects the class \\(C_k\\) with the highest posterior probability:\n",
        "\n",
        "\\[\n",
        "\\hat{C} = \\arg\\max_{C_k} \\left( P(C_k) \\prod_{i=1}^{n} P(x_i | C_k) \\right)\n",
        "\\]\n",
        "\n",
        "The class with the highest probability is assigned to the new data point.\n",
        "\n",
        "### Why is it Called \"Naïve\"?\n",
        "\n",
        "The \"Naïve\" part of **Naïve Bayes** comes from the **strong assumption** of **feature independence**. In real-world datasets, features are often correlated or dependent on each other, but the Naïve Bayes classifier assumes that each feature is independent of the others, given the class label.\n",
        "\n",
        "For example, if you're classifying emails as spam or not spam based on words in the email, Naïve Bayes assumes that the presence of each word is independent of the others, given whether the email is spam or not. In reality, however, some words might appear together more often (e.g., \"free\" and \"money\"), and their presence might be correlated, but Naïve Bayes ignores that.\n",
        "\n",
        "Despite this simplifying assumption of independence being unrealistic in many cases, **Naïve Bayes often performs surprisingly well**, especially when the dataset is large or when the independence assumption is roughly true for most features.\n",
        "\n",
        "### Types of Naïve Bayes Classifiers:\n",
        "\n",
        "There are different types of Naïve Bayes classifiers depending on the type of data (discrete or continuous):\n",
        "\n",
        "1. **Multinomial Naïve Bayes**: Used for discrete features, often used for text classification tasks where features are word counts or frequencies.\n",
        "   \n",
        "2. **Gaussian Naïve Bayes**: Used when the features are continuous. It assumes that the features follow a **Gaussian distribution** (normal distribution).\n",
        "   \n",
        "3. **Bernoulli Naïve Bayes**: Used for binary/Boolean features (e.g., presence or absence of a feature).\n",
        "\n",
        "### Advantages of Naïve Bayes:\n",
        "\n",
        "- **Simplicity**: The algorithm is easy to understand and implement.\n",
        "- **Efficiency**: Naïve Bayes is computationally efficient, especially with large datasets.\n",
        "- **Works well with categorical data**: It handles categorical features (like text classification) very well.\n",
        "- **Good for high-dimensional data**: It's effective in scenarios where the number of features is large, such as text classification.\n",
        "- **Fast training**: It has a fast training process and works well even with small datasets.\n",
        "\n",
        "### Disadvantages of Naïve Bayes:\n",
        "\n",
        "- **Independence assumption**: The main drawback is the assumption that features are independent. This can be unrealistic in many real-world scenarios.\n",
        "- **Zero probability problem**: If a feature value never appears in the training set for a given class, the model assigns zero probability to that class. This is typically handled by techniques like **Laplace smoothing**.\n",
        "- **Performance**: While Naïve Bayes is fast and efficient, its performance can degrade if the independence assumption is severely violated.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Imagine you have a dataset of emails with the features \"contains 'free'\" and \"contains 'money'.\" If you're trying to classify whether an email is spam or not, Naïve Bayes would calculate the probability of the email being spam or not, based on the presence or absence of these words, assuming the two features are independent.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The **Naïve Bayes classifier** is a **simple, probabilistic classification model** that uses Bayes' Theorem with an assumption of conditional independence between features. It is called \"Naïve\" because of this simplifying assumption, which, while unrealistic in many cases, still allows the algorithm to perform quite well in practice, especially when the independence assumption is approximately true or when the dataset is large."
      ],
      "metadata": {
        "id": "-hUEMVOVfKL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.13 What is Bayes’ Theorem?"
      ],
      "metadata": {
        "id": "vJkllJYwfaue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.13  **Bayes' Theorem** is a fundamental theorem in probability theory that describes how to update the probability of a hypothesis (or event) based on new evidence. It relates the conditional and marginal probabilities of random events, allowing you to revise your predictions as new data becomes available.\n",
        "\n",
        "Bayes' Theorem is named after **Thomas Bayes**, an 18th-century statistician, and it provides a way to calculate the **posterior probability** of an event, given prior knowledge and new evidence.\n",
        "\n",
        "### The Formula for Bayes' Theorem:\n",
        "\n",
        "The mathematical formulation of Bayes' Theorem is:\n",
        "\n",
        "\\[\n",
        "P(A | B) = \\frac{P(B | A) P(A)}{P(B)}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( P(A | B) \\) is the **posterior probability**: the probability of event \\( A \\) occurring given that event \\( B \\) has occurred.\n",
        "- \\( P(B | A) \\) is the **likelihood**: the probability of observing event \\( B \\) given that event \\( A \\) is true.\n",
        "- \\( P(A) \\) is the **prior probability**: the initial probability of event \\( A \\) before observing the evidence \\( B \\).\n",
        "- \\( P(B) \\) is the **evidence** or **marginal probability**: the total probability of event \\( B \\), which can be calculated as the sum of the likelihood of \\( B \\) under all possible conditions.\n",
        "\n",
        "### Intuitive Explanation:\n",
        "\n",
        "Bayes' Theorem provides a method for **updating** your belief about a hypothesis \\( A \\) after you observe some new evidence \\( B \\). The **prior probability** \\( P(A) \\) represents your initial belief about \\( A \\) before seeing the evidence, while the **likelihood** \\( P(B | A) \\) gives the probability of observing the evidence if the hypothesis is true. The theorem allows you to combine these values and obtain an updated (posterior) probability \\( P(A | B) \\) that takes the new evidence into account.\n",
        "\n",
        "### Example:\n",
        "\n",
        "Suppose you're a doctor trying to diagnose whether a patient has a particular disease. You have the following information:\n",
        "- \\( P(D) \\): The prior probability that a person has the disease is 1% (i.e., 1 out of 100 people).\n",
        "- \\( P(S | D) \\): The probability that a person with the disease shows a specific symptom is 80% (i.e., 80% of people with the disease have this symptom).\n",
        "- \\( P(S) \\): The probability that a person shows the symptom, regardless of whether they have the disease, is 20%.\n",
        "\n",
        "Now, if the patient shows the symptom, you can use Bayes' Theorem to calculate the probability that the patient actually has the disease:\n",
        "\n",
        "\\[\n",
        "P(D | S) = \\frac{P(S | D) P(D)}{P(S)}\n",
        "\\]\n",
        "\n",
        "Substituting the values:\n",
        "\n",
        "\\[\n",
        "P(D | S) = \\frac{(0.80) (0.01)}{0.20} = 0.04\n",
        "\\]\n",
        "\n",
        "So, the probability that the patient has the disease, given that they have the symptom, is **4%**.\n",
        "\n",
        "### Why Use Bayes' Theorem?\n",
        "\n",
        "Bayes' Theorem is widely used in statistics, machine learning, and decision theory for the following reasons:\n",
        "\n",
        "1. **Updating beliefs**: It allows you to update your probability estimate for an event as you gather new evidence.\n",
        "2. **Handling uncertainty**: Bayes' Theorem is particularly useful in situations where uncertainty is involved and you need to refine predictions as new data comes in.\n",
        "3. **Decision making**: It helps make more informed decisions by providing a framework for considering both prior knowledge and new evidence.\n",
        "4. **Foundations for many models**: Many classification algorithms (like Naïve Bayes) and statistical models are built upon Bayes' Theorem.\n",
        "\n",
        "### Key Concepts:\n",
        "\n",
        "1. **Prior Probability** (\\( P(A) \\)): The initial belief about an event or hypothesis before new evidence is observed.\n",
        "   \n",
        "2. **Likelihood** (\\( P(B | A) \\)): The probability of observing the evidence (B) given that the hypothesis (A) is true.\n",
        "\n",
        "3. **Posterior Probability** (\\( P(A | B) \\)): The updated belief about the hypothesis (A) after observing the evidence (B).\n",
        "\n",
        "4. **Marginal Probability** (\\( P(B) \\)): The total probability of observing the evidence (B) across all possible hypotheses.\n",
        "\n",
        "### Summary:\n",
        "\n",
        "Bayes' Theorem provides a way to **update your knowledge** about a hypothesis as new evidence becomes available. It helps to revise predictions based on prior information and current observations, making it a powerful tool in probabilistic reasoning, decision-making, and machine learning tasks."
      ],
      "metadata": {
        "id": "IGV7kD7HflTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.14 Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes:"
      ],
      "metadata": {
        "id": "8FTEptv7gDgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.14 The **Naïve Bayes classifier** is a family of probabilistic classifiers based on **Bayes' Theorem**, and it assumes that the features are **conditionally independent** given the class label. There are different variants of the Naïve Bayes classifier, each suited for different types of data. The three most commonly used variants are:\n",
        "\n",
        "1. **Gaussian Naïve Bayes**\n",
        "2. **Multinomial Naïve Bayes**\n",
        "3. **Bernoulli Naïve Bayes**\n",
        "\n",
        "Each variant makes different assumptions about how the features (or attributes) of the data are distributed. Let's break down the differences between these three types:\n",
        "\n",
        "### 1. **Gaussian Naïve Bayes (GNB)**\n",
        "\n",
        "#### Assumption:\n",
        "- **Gaussian Naïve Bayes** assumes that the features are **continuous** and follow a **Gaussian (normal) distribution**. This means that for each class, the distribution of each feature is modeled as a normal distribution with a mean and standard deviation specific to that class.\n",
        "  \n",
        "#### Formula:\n",
        "For each feature \\( x_i \\) in the data, the probability of observing \\( x_i \\) given the class \\( C_k \\) is modeled as a **Gaussian distribution**:\n",
        "\\[\n",
        "P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma_k^2}} \\exp\\left(-\\frac{(x_i - \\mu_k)^2}{2\\sigma_k^2}\\right)\n",
        "\\]\n",
        "Where:\n",
        "- \\( \\mu_k \\) is the mean of the feature \\( x_i \\) for class \\( C_k \\),\n",
        "- \\( \\sigma_k^2 \\) is the variance of the feature \\( x_i \\) for class \\( C_k \\).\n",
        "\n",
        "#### When to Use:\n",
        "- **Continuous data**: Suitable for problems where the features are continuous (e.g., heights, weights, or any measurements that are real-valued).\n",
        "  \n",
        "#### Example:\n",
        "- Predicting whether a patient has a certain disease based on continuous features like blood pressure, age, or cholesterol levels.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Multinomial Naïve Bayes (MNB)**\n",
        "\n",
        "#### Assumption:\n",
        "- **Multinomial Naïve Bayes** is used when the features are **discrete** and represent counts or frequencies of events. The most common use case is **text classification**, where the features represent word counts or word frequencies.\n",
        "  \n",
        "#### Formula:\n",
        "Given that the features represent counts, the likelihood of observing a word \\( x_i \\) given the class \\( C_k \\) is modeled as a **multinomial distribution**:\n",
        "\\[\n",
        "P(x_i | C_k) = \\frac{(N_{ik} + \\alpha)}{(N_k + \\alpha \\cdot V)}\n",
        "\\]\n",
        "Where:\n",
        "- \\( N_{ik} \\) is the count of feature \\( x_i \\) (e.g., the count of a word) in class \\( C_k \\),\n",
        "- \\( N_k \\) is the total count of all features (e.g., total word count) in class \\( C_k \\),\n",
        "- \\( V \\) is the vocabulary size (total number of unique features/words),\n",
        "- \\( \\alpha \\) is a smoothing parameter (often Laplace smoothing).\n",
        "\n",
        "#### When to Use:\n",
        "- **Discrete data with counts**: This model is typically used for text classification or when dealing with other types of data where the features are counts (e.g., number of occurrences of certain events).\n",
        "  \n",
        "#### Example:\n",
        "- **Text classification**: Classifying documents into categories based on word frequencies or counts (e.g., classifying emails as spam or not spam based on word counts).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Bernoulli Naïve Bayes (BNB)**\n",
        "\n",
        "#### Assumption:\n",
        "- **Bernoulli Naïve Bayes** is used when the features are **binary** (i.e., they take only two possible values, usually 0 or 1). This variant assumes that each feature represents the presence or absence of a particular attribute, and the model is based on a **Bernoulli distribution**.\n",
        "\n",
        "#### Formula:\n",
        "For each feature \\( x_i \\), the probability of observing \\( x_i = 1 \\) (presence of the feature) given the class \\( C_k \\) is modeled as:\n",
        "\\[\n",
        "P(x_i | C_k) = P(x_i = 1 | C_k)^{x_i} (1 - P(x_i = 1 | C_k))^{1 - x_i}\n",
        "\\]\n",
        "Where:\n",
        "- \\( P(x_i = 1 | C_k) \\) is the probability of feature \\( x_i \\) being present in class \\( C_k \\),\n",
        "- \\( x_i \\) is the binary value indicating whether the feature is present (1) or absent (0).\n",
        "\n",
        "#### When to Use:\n",
        "- **Binary features**: This model is used when the features are binary, i.e., they represent the presence or absence of something (e.g., whether a word appears in a document or not).\n",
        "  \n",
        "#### Example:\n",
        "- **Document classification** where the presence or absence of certain keywords is used as features to classify text (e.g., whether an email is spam or not, based on the presence of certain words like \"free\", \"offer\", etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Differences:\n",
        "\n",
        "| **Variant**               | **Feature Type**         | **Distribution Assumption**                 | **Typical Use Case**                                  |\n",
        "|---------------------------|--------------------------|----------------------------------------------|------------------------------------------------------|\n",
        "| **Gaussian Naïve Bayes**   | Continuous               | Gaussian (normal) distribution               | Continuous features (e.g., height, weight, temperature)|\n",
        "| **Multinomial Naïve Bayes**| Discrete (counts)        | Multinomial distribution (counts/frequencies) | Text classification (word counts or frequencies)      |\n",
        "| **Bernoulli Naïve Bayes**  | Binary (0 or 1)          | Bernoulli distribution (binary outcomes)     | Binary features (presence/absence of attributes)     |\n",
        "\n",
        "### Choosing the Right Model:\n",
        "- **Gaussian Naïve Bayes** is best when the features are continuous and assumed to follow a normal distribution.\n",
        "- **Multinomial Naïve Bayes** is ideal for **count-based** features (e.g., word counts in text classification).\n",
        "- **Bernoulli Naïve Bayes** is used when the features are binary (e.g., word presence or absence in text classification).\n",
        "\n",
        "Understanding the type of data you're working with is crucial for choosing the appropriate Naïve Bayes variant."
      ],
      "metadata": {
        "id": "VcgWhkE2gMrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.15 When should you use Gaussian Naïve Bayes over other variants?\n"
      ],
      "metadata": {
        "id": "YKQio3uvghK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.15 You should use **Gaussian Naïve Bayes** (GNB) over other variants like **Multinomial Naïve Bayes (MNB)** or **Bernoulli Naïve Bayes (BNB)** when:\n",
        "\n",
        "### 1. **Features are Continuous and Follow a Gaussian Distribution**\n",
        "   - **Gaussian Naïve Bayes** assumes that the features are continuous and follow a **normal (Gaussian) distribution**. If your data consists of continuous numerical features (e.g., age, height, temperature, or financial data), and these features are approximately **normally distributed** for each class, then Gaussian Naïve Bayes is a natural choice.\n",
        "\n",
        "   **Example:**\n",
        "   - Predicting whether a customer will default on a loan based on continuous variables like income, loan amount, credit score, etc., that follow a normal distribution.\n",
        "\n",
        "### 2. **When the Data is Not Discrete or Binary**\n",
        "   - **Multinomial Naïve Bayes** works well with discrete **count-based data** (like word frequencies in text classification), and **Bernoulli Naïve Bayes** is best for binary data (presence/absence of features). If your features are **continuous**, then **Gaussian Naïve Bayes** is more appropriate than these variants, which are tailored for discrete or binary data.\n",
        "\n",
        "   **Example:**\n",
        "   - In cases where your features are measurements (like weight, height, etc.) and are not counts or binary values.\n",
        "\n",
        "### 3. **If You Believe the Features are Normally Distributed within Each Class**\n",
        "   - Gaussian Naïve Bayes works well when you can reasonably assume that the features follow a **normal distribution**. If you expect the values of your features (given the class) to cluster around a mean with a certain standard deviation, then GNB will model this well.\n",
        "   \n",
        "   **Example:**\n",
        "   - Predicting the probability of an event (e.g., disease diagnosis) based on continuous clinical measurements that are likely to be normally distributed within each class.\n",
        "\n",
        "### 4. **In Situations with Small Data**\n",
        "   - **Gaussian Naïve Bayes** is often more **robust** to small datasets compared to other models. Since it only estimates the mean and variance of each feature for each class (which are easy to calculate), it doesn’t require a lot of data to work well. This is beneficial in scenarios where data may be limited.\n",
        "\n",
        "   **Example:**\n",
        "   - When you have a small sample size for a particular class or category but still need to classify new data.\n",
        "\n",
        "### 5. **When You Need a Simple, Fast, and Interpretable Model**\n",
        "   - Gaussian Naïve Bayes is computationally efficient, easy to implement, and interpretable. The model’s simplicity makes it a good option when you need fast results or a model that you can explain easily. It’s especially suitable for **baseline models** when you want a quick comparison to more complex models.\n",
        "\n",
        "   **Example:**\n",
        "   - Building a quick, interpretable model for exploratory analysis, or as a baseline model before trying more complex classifiers.\n",
        "\n",
        "---\n",
        "\n",
        "### When **NOT** to Use Gaussian Naïve Bayes:\n",
        "\n",
        "- **When your features are not continuous or not approximately normally distributed**:\n",
        "   If your data is **discrete** (like counts or categories) or **binary** (like presence/absence of a feature), **Multinomial Naïve Bayes** or **Bernoulli Naïve Bayes** would be more appropriate than Gaussian Naïve Bayes.\n",
        "  \n",
        "  **Example:**\n",
        "   - Text classification where features are the counts or frequencies of words (use **Multinomial Naïve Bayes**).\n",
        "   - Binary classification with features representing whether certain events or words occur (use **Bernoulli Naïve Bayes**).\n",
        "\n",
        "- **If the features are highly skewed or have outliers**:\n",
        "   If your features do not follow a **normal distribution**, or have significant **skewness** or **outliers**, then Gaussian Naïve Bayes might not perform well, since it heavily relies on the assumption that the data follows a normal distribution. In such cases, you might want to explore other models that do not assume this, like decision trees, random forests, or support vector machines (SVM).\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "Use **Gaussian Naïve Bayes** when:\n",
        "- Your features are continuous.\n",
        "- The features approximately follow a **normal distribution**.\n",
        "- You need a **simple, interpretable, and fast** model.\n",
        "- Your dataset is small and you want a model that works well with limited data.\n",
        "\n",
        "If your data is **discrete**, **count-based**, or **binary**, then **Multinomial Naïve Bayes** or **Bernoulli Naïve Bayes** might be more appropriate."
      ],
      "metadata": {
        "id": "Wwyrp9wtgqPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.16 What are the key assumptions made by Naïve Bayes?"
      ],
      "metadata": {
        "id": "0qBxdqP2g-tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.16 The **Naïve Bayes** classifier is based on a few key **assumptions** that simplify the classification process. Understanding these assumptions helps clarify why the model works well in certain scenarios and might struggle in others. Here are the key assumptions made by Naïve Bayes:\n",
        "\n",
        "### 1. **Conditional Independence of Features Given the Class**\n",
        "   - The most important assumption in Naïve Bayes is that the features are **conditionally independent** given the class label.\n",
        "   - **What it means**: For a given class label, the presence or absence of a feature is assumed to be independent of the presence or absence of any other feature. In other words, the features do not influence each other once the class label is known.\n",
        "   \n",
        "   **Example**:\n",
        "   - If you're classifying an email as \"spam\" or \"not spam,\" Naïve Bayes assumes that the presence of the word \"free\" in the email is independent of the presence of the word \"offer,\" given the class label (\"spam\" or \"not spam\"). This is clearly an oversimplification, as in reality, certain words may often occur together, but the model ignores these correlations.\n",
        "\n",
        "### 2. **The Class Conditional Distributions are Known**\n",
        "   - Naïve Bayes assumes that the **conditional distribution of each feature** given the class is known. This means that for each feature \\( x_i \\), we know the distribution of \\( x_i \\) for each class \\( C_k \\). For example, in **Gaussian Naïve Bayes**, this distribution is assumed to be Gaussian (normal), whereas in **Multinomial Naïve Bayes**, the distribution is assumed to be multinomial.\n",
        "   \n",
        "   **Example**:\n",
        "   - For Gaussian Naïve Bayes, you assume that the features (e.g., height, age) within each class follow a normal (Gaussian) distribution, with their own mean and variance for each class.\n",
        "   \n",
        "### 3. **The Features are Conditioned on the Class Label**\n",
        "   - The model assumes that **each feature** (or attribute) is dependent only on the **class label**, and not on any other feature. This means that while the features may be correlated in reality, the Naïve Bayes classifier assumes that each feature contributes independently to the likelihood of a class.\n",
        "   \n",
        "   **Example**:\n",
        "   - In a spam email classifier, if the email contains both \"free\" and \"offer,\" Naïve Bayes assumes that the likelihood of the email being spam due to \"free\" is independent of the likelihood of it being spam due to \"offer\" (given the spam class).\n",
        "\n",
        "### 4. **Class Conditional Independence Does Not Account for Inter-feature Dependencies**\n",
        "   - This assumption implies that Naïve Bayes doesn't account for **interdependencies** between features. In practice, this can lead to suboptimal performance when there are **strong correlations** or **dependencies** between features.\n",
        "   \n",
        "   **Example**:\n",
        "   - If two features are highly correlated, such as \"wet ground\" and \"raining,\" Naïve Bayes treats them as independent, which may lead to less accurate predictions compared to models that can capture feature dependencies, like decision trees or support vector machines (SVM).\n",
        "\n",
        "### 5. **The Prior Probabilities of Classes are Known**\n",
        "   - Naïve Bayes requires the **prior probabilities** of each class (i.e., how likely each class is before seeing any data). These priors are typically estimated from the training data by calculating the proportion of each class.\n",
        "   \n",
        "   **Example**:\n",
        "   - In spam detection, the prior probability of an email being spam is calculated as the fraction of spam emails in the training dataset.\n",
        "\n",
        "### 6. **Feature Relevance Is Assumed**\n",
        "   - The Naïve Bayes classifier assumes that all features are **equally relevant** when making predictions. It doesn’t give any special weight to any specific feature, which can be limiting if some features are more important than others for classification.\n",
        "   \n",
        "   **Example**:\n",
        "   - If you're classifying emails as spam or not spam, Naïve Bayes treats every word equally, even though some words (e.g., \"free\" or \"limited-time offer\") might be much stronger indicators of spam than others (e.g., \"and\" or \"the\").\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Key Assumptions in Naïve Bayes:\n",
        "\n",
        "1. **Conditional Independence**: Features are independent given the class label.\n",
        "2. **Class Conditional Distributions**: The distribution of each feature is known and typically follows a specific distribution (e.g., Gaussian or multinomial).\n",
        "3. **Feature Conditioning**: Features depend only on the class label, not on each other.\n",
        "4. **Inter-feature Dependencies Ignored**: Naïve Bayes does not account for relationships or dependencies between features.\n",
        "5. **Prior Probabilities**: The prior probabilities of each class are known or can be estimated.\n",
        "6. **Equal Feature Relevance**: All features are assumed to be equally important in the classification.\n",
        "\n",
        "### Why \"Naïve\"?\n",
        "The model is called \"naïve\" because it makes the **unrealistic** assumption of **conditional independence** between features. In reality, many features are likely to be correlated with each other, and ignoring these correlations can lead to suboptimal performance. Despite this, Naïve Bayes often works surprisingly well, especially in high-dimensional spaces like text classification, where the conditional independence assumption can often be a reasonable approximation."
      ],
      "metadata": {
        "id": "VYHd5ROmhITT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.17 What are the advantages and disadvantages of Naïve Bayes?\n"
      ],
      "metadata": {
        "id": "IOseXJQGhYyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.17 The **Naïve Bayes** classifier is a simple and effective probabilistic model that is widely used for classification tasks. However, like any model, it has its strengths and weaknesses. Below are the key **advantages** and **disadvantages** of the Naïve Bayes algorithm.\n",
        "\n",
        "### **Advantages of Naïve Bayes**\n",
        "\n",
        "1. **Simple and Fast**:\n",
        "   - Naïve Bayes is relatively easy to implement and computationally efficient. It can be trained and tested very quickly, making it an attractive choice for large datasets and real-time applications.\n",
        "\n",
        "2. **Works Well with High-Dimensional Data**:\n",
        "   - Naïve Bayes is especially effective when dealing with high-dimensional data, such as in **text classification** (e.g., spam detection, sentiment analysis). For example, in text classification, each word can be treated as a feature, and Naïve Bayes handles large feature spaces efficiently.\n",
        "\n",
        "3. **Performs Well with Small Datasets**:\n",
        "   - The model is less prone to overfitting when the dataset is small, especially when the number of features is large. This is because it makes strong independence assumptions, which can simplify the learning process and prevent the model from becoming overly complex.\n",
        "\n",
        "4. **Handles Missing Data Well**:\n",
        "   - Naïve Bayes can perform well even if some of the features are missing, as it calculates probabilities for each feature independently. If a feature value is missing for a given instance, the classifier simply omits it in the calculation without much impact on the result.\n",
        "\n",
        "5. **Interpretable**:\n",
        "   - Naïve Bayes is interpretable because it provides the **probabilities** of each class, which allows users to understand how the model arrives at its decision. This makes it easy to explain predictions to stakeholders.\n",
        "\n",
        "6. **Works Well with Binary and Multi-class Classification**:\n",
        "   - Naïve Bayes can be used for both binary classification (e.g., spam vs. not spam) and multi-class classification problems (e.g., classifying text into multiple categories).\n",
        "\n",
        "7. **Good Performance with Text Data**:\n",
        "   - Naïve Bayes is particularly well-suited for text classification problems, especially when the features are binary or represent counts of occurrences (e.g., word frequencies), as in **Multinomial Naïve Bayes**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Disadvantages of Naïve Bayes**\n",
        "\n",
        "1. **Strong Independence Assumption (Conditional Independence)**:\n",
        "   - The most significant disadvantage of Naïve Bayes is its assumption that all features are **conditionally independent**, given the class label. In real-world data, features are often correlated, and this assumption can lead to suboptimal performance if the features are highly dependent on each other.\n",
        "\n",
        "   **Example**:\n",
        "   - In spam classification, words like \"offer\" and \"free\" often appear together in spam emails. Naïve Bayes assumes that these words are independent, which may not always be true and can hurt performance.\n",
        "\n",
        "2. **Limited to Known Distributions**:\n",
        "   - Naïve Bayes assumes that the data follows specific distributions (e.g., Gaussian for **Gaussian Naïve Bayes**, multinomial for **Multinomial Naïve Bayes**, or Bernoulli for **Bernoulli Naïve Bayes**). If the data does not follow the assumed distribution, the model's accuracy can suffer.\n",
        "\n",
        "3. **Poor Performance with Highly Correlated Features**:\n",
        "   - If there are highly correlated features, Naïve Bayes might not perform well because it treats each feature as independent, ignoring any interdependencies between them. This can reduce the model’s ability to capture important patterns in the data.\n",
        "\n",
        "4. **Difficulty with Numerical Data**:\n",
        "   - Naïve Bayes can struggle with **numerical data** that does not follow a Gaussian distribution. While Gaussian Naïve Bayes assumes a normal distribution for continuous features, if the actual data is skewed or has outliers, this assumption can lead to poor performance.\n",
        "\n",
        "5. **Sensitivity to Imbalanced Data**:\n",
        "   - Naïve Bayes can be sensitive to imbalanced datasets, where one class is much more prevalent than the other. In such cases, the model may become biased toward the majority class, resulting in inaccurate predictions for the minority class. Techniques like **class weighting** or **oversampling** can help, but the model may still perform poorly if class imbalances are extreme.\n",
        "\n",
        "6. **Assumption of Feature Relevance**:\n",
        "   - Naïve Bayes assumes that all features are **equally relevant** for classification. In many real-world applications, however, some features may be much more important than others. More complex models (e.g., decision trees, random forests, or SVMs) may be able to better account for feature importance.\n",
        "\n",
        "7. **Not Suitable for Regression Problems**:\n",
        "   - Naïve Bayes is designed for classification tasks, not regression. While it can be adapted for regression using techniques like **Gaussian Naïve Bayes** with continuous data, it is not inherently suited for predicting continuous numerical values.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Advantages and Disadvantages**\n",
        "\n",
        "| **Advantages**                                       | **Disadvantages**                                    |\n",
        "|------------------------------------------------------|------------------------------------------------------|\n",
        "| Simple and fast to implement and train               | Assumes conditional independence of features         |\n",
        "| Works well with high-dimensional and sparse data     | Sensitive to highly correlated features              |\n",
        "| Performs well with small datasets                    | Limited to known distributions (Gaussian, Multinomial, etc.) |\n",
        "| Can handle missing data                              | May perform poorly with skewed or outlier-prone data |\n",
        "| Easily interpretable (provides probabilities)        | Poor performance on imbalanced datasets              |\n",
        "| Well-suited for binary and multi-class classification| Assumes equal feature relevance (no feature weighting) |\n",
        "| Great for text classification (e.g., spam detection) | Not suited for regression problems                   |\n",
        "\n",
        "### **When to Use Naïve Bayes**:\n",
        "- **Text classification tasks**, such as spam filtering, sentiment analysis, and document categorization.\n",
        "- When you have **high-dimensional data** with **many features** (e.g., in text classification, where the number of words can be very large).\n",
        "- **Simple and fast baseline models** when you need a quick solution to compare with more complex models.\n",
        "- Situations where you have **small datasets** and want a model that is less prone to overfitting.\n",
        "\n",
        "### **When to Avoid Naïve Bayes**:\n",
        "- If your features are **strongly correlated**, Naïve Bayes may not perform well.\n",
        "- If your data does not follow the **assumed distributions** (e.g., Gaussian for continuous data or multinomial for text data).\n",
        "- When dealing with **imbalanced datasets**, especially when the class imbalance is extreme.\n",
        "\n",
        "In practice, Naïve Bayes can often perform surprisingly well, even with its simplifying assumptions, especially for tasks like text classification. However, it's important to understand the model's limitations and consider using more complex models when necessary."
      ],
      "metadata": {
        "id": "_cRMU0u_hiQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.18 Why is Naïve Bayes a good choice for text classification?\n"
      ],
      "metadata": {
        "id": "ImxdDSQlhz_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.18 **Naïve Bayes** is a popular choice for **text classification** tasks due to several reasons that align well with the characteristics of text data. Here’s why Naïve Bayes works well for text classification:\n",
        "\n",
        "### 1. **High Dimensionality of Text Data**\n",
        "   - **Text data** typically has **high-dimensional feature spaces**, meaning each document or piece of text can have many features (e.g., individual words or n-grams). Naïve Bayes is efficient at handling high-dimensional data, making it particularly useful when you have a large number of features (like vocabulary size in text data).\n",
        "   - For example, in **document classification**, each unique word (or sequence of words) in a document becomes a feature. Even with thousands or millions of words, Naïve Bayes can still perform well due to its simplicity and efficiency.\n",
        "\n",
        "### 2. **Works Well with Sparse Data**\n",
        "   - **Text data is often sparse**, meaning most of the features (words) in a document may not appear in most other documents. Naïve Bayes works effectively with sparse datasets because it independently considers the presence (or absence) of each word in the document, without being influenced by the absence of other words.\n",
        "   - **Example**: If you have a dataset of news articles, most articles won’t use all words in the vocabulary, making it sparse. Naïve Bayes can still perform well because it doesn’t require all features (words) to be present.\n",
        "\n",
        "### 3. **Simple and Fast**\n",
        "   - Naïve Bayes is computationally efficient, requiring minimal time to train and classify documents. Text classification tasks, such as **spam detection** or **sentiment analysis**, often involve large datasets, and the speed of Naïve Bayes makes it a great choice for handling such tasks efficiently.\n",
        "   - This is particularly useful for real-time applications where fast predictions are required, such as classifying incoming emails as spam or not.\n",
        "\n",
        "### 4. **Conditional Independence Assumption Works Well for Text**\n",
        "   - In text classification, the **conditional independence assumption** of Naïve Bayes (where each word is assumed to contribute independently to the likelihood of the class) is often a reasonable approximation. While words in a document are often correlated, treating them as independent features works surprisingly well in practice.\n",
        "   - For instance, in a spam classification task, even though certain words (like “free” and “offer”) may often appear together, Naïve Bayes treats them independently and still produces a reliable classification model.\n",
        "\n",
        "### 5. **Effective for Multi-class Classification**\n",
        "   - Naïve Bayes is inherently well-suited for **multi-class classification** tasks. In text classification, you often need to classify documents into multiple categories (e.g., topics such as politics, sports, and entertainment). Naïve Bayes works well in these multi-class scenarios because it models the probability of each class and selects the class with the highest probability.\n",
        "\n",
        "### 6. **Handles Unseen Words (With Smoothing)**\n",
        "   - In text classification, new documents may contain words that were not present in the training data. **Laplace smoothing** (or additive smoothing) can be applied to Naïve Bayes to handle **unseen words** (i.e., words that didn’t appear in the training set).\n",
        "   - For example, in **Multinomial Naïve Bayes**, when calculating probabilities of words, a small constant is added to avoid zero probability for unseen words. This ensures that the model can still make predictions even when encountering new vocabulary.\n",
        "\n",
        "### 7. **Works Well with Binary and Count Features**\n",
        "   - Text classification often involves **binary features** (e.g., whether a word appears or not in a document) or **count-based features** (e.g., the frequency of a word in a document). Naïve Bayes handles these types of features naturally:\n",
        "     - **Bernoulli Naïve Bayes** is used when features are binary (word presence/absence).\n",
        "     - **Multinomial Naïve Bayes** is used when features are counts (word frequencies).\n",
        "\n",
        "   This makes Naïve Bayes ideal for tasks like **spam detection**, where you look for the presence or frequency of certain keywords in the email.\n",
        "\n",
        "### 8. **Interpretable and Probabilistic**\n",
        "   - Naïve Bayes provides **probabilistic outputs**, meaning it gives the probability that a given document belongs to each class. This is useful for text classification tasks where you might want to not only predict a class label (e.g., spam or not spam) but also understand how confident the model is in that prediction.\n",
        "   - The model's **simplicity** also makes it highly interpretable: the contribution of each feature (word) to the classification decision can be easily understood and explained.\n",
        "\n",
        "---\n",
        "\n",
        "### Example Use Cases in Text Classification:\n",
        "1. **Spam Filtering**:\n",
        "   - In a spam detection task, words like \"free\", \"offer\", and \"limited-time\" are common in spam emails. Naïve Bayes can use these words to classify new emails based on their occurrence.\n",
        "\n",
        "2. **Sentiment Analysis**:\n",
        "   - Naïve Bayes can classify movie reviews, tweets, or other text into categories like positive, negative, or neutral based on the words that appear in the text.\n",
        "\n",
        "3. **Topic Categorization**:\n",
        "   - In news article categorization, Naïve Bayes can classify articles into different topics (e.g., politics, sports, technology) by learning the likelihood of words associated with each topic.\n",
        "\n",
        "4. **Language Detection**:\n",
        "   - Naïve Bayes can be used to classify text based on the language it’s written in by learning which words are more likely to appear in different languages.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary: Why Naïve Bayes is Good for Text Classification\n",
        "- **Efficient for high-dimensional, sparse data**: Text data typically has many features (words), and Naïve Bayes works well with such data.\n",
        "- **Simple and fast**: It is computationally efficient and easy to train, making it suitable for large datasets.\n",
        "- **Effective with count and binary features**: It works well with features like word frequencies or binary word presence, common in text classification tasks.\n",
        "- **Good performance with the conditional independence assumption**: While this assumption is often unrealistic, it works surprisingly well in text classification, where features (words) are somewhat independent.\n",
        "- **Probabilistic output**: Naïve Bayes not only provides class labels but also the probabilities, making it interpretable and useful for applications needing probabilistic predictions.\n",
        "  \n",
        "These factors make Naïve Bayes a great starting point and baseline model for text classification problems, especially when you're dealing with large amounts of text data, such as in email filtering or sentiment analysis."
      ],
      "metadata": {
        "id": "l8Ic9fpliYvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.19 Compare SVM and Naïve Bayes for classification tasks:"
      ],
      "metadata": {
        "id": "ocE63_Nwilxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.19 Support Vector Machines (SVM) and Naïve Bayes are both popular algorithms for classification tasks, but they differ significantly in their approach, assumptions, and performance characteristics. Here's a detailed comparison:\n",
        "\n",
        "### 1. **Model Type**\n",
        "- **SVM (Support Vector Machine):**\n",
        "  - SVM is a **discriminative** model, meaning it focuses on finding the decision boundary that best separates classes in the feature space. It tries to find the **hyperplane** that maximizes the margin between classes, which is the distance between the closest data points (support vectors) of each class.\n",
        "  \n",
        "- **Naïve Bayes:**\n",
        "  - Naïve Bayes is a **generative** model, meaning it models the joint probability distribution of the features and the classes. It makes the assumption that the features are conditionally independent given the class label, which simplifies the calculation of probabilities.\n",
        "\n",
        "### 2. **Mathematical Foundation**\n",
        "- **SVM:**\n",
        "  - SVM is based on **optimization** theory. It solves a quadratic optimization problem to find the optimal hyperplane. The idea is to maximize the margin between the classes while minimizing classification error. The model can use **kernels** to transform non-linear problems into a higher-dimensional space where a linear separation might be possible.\n",
        "  \n",
        "- **Naïve Bayes:**\n",
        "  - Naïve Bayes is based on **Bayes' theorem**, and the \"naïve\" assumption is that the features are independent given the class. It computes the posterior probability of each class using the likelihood of the features given the class and the prior probabilities of the classes. The class with the highest posterior probability is chosen as the predicted class.\n",
        "\n",
        "### 3. **Assumptions**\n",
        "- **SVM:**\n",
        "  - SVM makes **no assumptions** about the underlying distribution of the data. It works well even when the data does not follow a specific distribution (e.g., Gaussian distribution).\n",
        "\n",
        "- **Naïve Bayes:**\n",
        "  - Naïve Bayes assumes that **features are conditionally independent** given the class. This assumption is often unrealistic in real-world data, but the model still tends to perform well even if this assumption is violated.\n",
        "\n",
        "### 4. **Performance with High-Dimensional Data**\n",
        "- **SVM:**\n",
        "  - SVM can handle **high-dimensional data** effectively, especially with the use of kernel functions. It is widely used in areas like text classification and bioinformatics, where the feature space is large and sparse (e.g., TF-IDF vectors in text).\n",
        "  \n",
        "- **Naïve Bayes:**\n",
        "  - Naïve Bayes also works well with **high-dimensional data**, as it is computationally efficient and doesn’t require much training time. However, its performance can degrade if the independence assumption is severely violated.\n",
        "\n",
        "### 5. **Speed and Complexity**\n",
        "- **SVM:**\n",
        "  - **Training:** SVMs can be computationally expensive, especially with large datasets, because they involve solving a quadratic optimization problem. The complexity increases with the number of data points and features.\n",
        "  - **Prediction:** Once the model is trained, predicting is relatively fast, but it can still be slower compared to Naïve Bayes for very large datasets.\n",
        "\n",
        "- **Naïve Bayes:**\n",
        "  - **Training:** Naïve Bayes is very **fast** to train because it only requires calculating probabilities from the training data, which is computationally inexpensive.\n",
        "  - **Prediction:** Naïve Bayes is also very fast for prediction, making it ideal for applications requiring quick results.\n",
        "\n",
        "### 6. **Handling Non-Linearity**\n",
        "- **SVM:**\n",
        "  - SVM can handle **non-linear** relationships between features by using kernels (like the radial basis function (RBF) kernel). This allows SVM to create complex decision boundaries that are not restricted to linear separation.\n",
        "\n",
        "- **Naïve Bayes:**\n",
        "  - Naïve Bayes doesn’t naturally handle **non-linear** relationships well because it assumes independence between features. If the true relationship is non-linear or if features are highly correlated, Naïve Bayes may not perform well.\n",
        "\n",
        "### 7. **Outliers Sensitivity**\n",
        "- **SVM:**\n",
        "  - SVM is relatively **sensitive to outliers** because it tries to maximize the margin based on the support vectors, and outliers can distort this margin. However, using a soft margin (allowing some misclassification) can make SVM more robust to outliers.\n",
        "  \n",
        "- **Naïve Bayes:**\n",
        "  - Naïve Bayes is generally **less sensitive to outliers** because it relies on the probability distributions, but extreme outliers can still affect the model's performance, especially when they heavily influence the estimates of conditional probabilities.\n",
        "\n",
        "### 8. **Interpretability**\n",
        "- **SVM:**\n",
        "  - SVMs, especially with kernels, are often considered **less interpretable** because they create complex decision boundaries. The model is harder to explain, especially when the kernel trick is used to map data into a higher-dimensional space.\n",
        "\n",
        "- **Naïve Bayes:**\n",
        "  - Naïve Bayes is **more interpretable** because it provides clear probabilities for each class and makes it easy to understand the relationship between each feature and the target class.\n",
        "\n",
        "### 9. **Use Cases**\n",
        "- **SVM:**\n",
        "  - Best suited for problems with a **large number of features** and when the classes are not easily separable. Common applications include text classification, image classification, and bioinformatics.\n",
        "  \n",
        "- **Naïve Bayes:**\n",
        "  - Works well in situations where the features are somewhat independent and the **assumptions hold**. It is often used in **spam filtering**, document classification, and real-time predictions where speed is important.\n",
        "\n",
        "### 10. **Strengths and Weaknesses**\n",
        "- **SVM Strengths:**\n",
        "  - Handles high-dimensional data well.\n",
        "  - Effective in non-linear problems (with kernel trick).\n",
        "  - Robust in finding a good decision boundary.\n",
        "  \n",
        "- **SVM Weaknesses:**\n",
        "  - Computationally expensive for large datasets.\n",
        "  - Sensitive to outliers.\n",
        "  - Requires careful parameter tuning (e.g., regularization, kernel choice).\n",
        "\n",
        "- **Naïve Bayes Strengths:**\n",
        "  - Simple and fast to train.\n",
        "  - Works well with small datasets and high-dimensional feature spaces.\n",
        "  - Robust to irrelevant features.\n",
        "  \n",
        "- **Naïve Bayes Weaknesses:**\n",
        "  - Assumes independence between features, which is often unrealistic.\n",
        "  - Performance degrades when the independence assumption is violated.\n",
        "  \n",
        "### Conclusion:\n",
        "- **SVM** is a powerful and flexible algorithm, especially suited for complex and non-linear classification tasks but may be computationally expensive for large datasets.\n",
        "- **Naïve Bayes**, on the other hand, is simpler and faster, making it a good choice for problems where the independence assumption is reasonable, or for real-time prediction tasks.\n",
        "\n",
        "The choice between SVM and Naïve Bayes depends on the dataset characteristics and the specific task requirements."
      ],
      "metadata": {
        "id": "NJl50pdVivBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.20 How does Laplace Smoothing help in Naïve Bayes?\n"
      ],
      "metadata": {
        "id": "aXWGtV2JjBkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ANS.20 **Laplace Smoothing** (also known as **additive smoothing**) is a technique used in **Naïve Bayes** to address the issue of **zero probabilities** in the probability estimates when certain feature-class combinations are absent in the training data.\n",
        "\n",
        "Here’s how Laplace Smoothing helps in Naïve Bayes:\n",
        "\n",
        "### 1. **Handling Zero Probabilities**\n",
        "In Naïve Bayes, the classification process relies on calculating the conditional probabilities of the features given the class. For instance, when computing the likelihood of a feature \\(x_i\\) given a class \\(C_k\\), we estimate:\n",
        "\n",
        "\\[\n",
        "P(x_i | C_k) = \\frac{\\text{count}(x_i, C_k)}{\\text{count}(C_k)}\n",
        "\\]\n",
        "\n",
        "However, if a feature \\(x_i\\) doesn't appear in the training data for a particular class \\(C_k\\), the probability \\(P(x_i | C_k)\\) becomes zero. This can severely affect the prediction since any feature with a zero probability would cause the product of all probabilities to become zero, leading to incorrect predictions.\n",
        "\n",
        "### 2. **Laplace Smoothing Formula**\n",
        "Laplace smoothing modifies the probability estimation to ensure that no probability is exactly zero. It does this by adding a small constant (usually 1) to the numerator and adjusting the denominator accordingly. The formula with Laplace smoothing is:\n",
        "\n",
        "\\[\n",
        "P(x_i | C_k) = \\frac{\\text{count}(x_i, C_k) + 1}{\\text{count}(C_k) + V}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **count(x_i, C_k)** is the frequency of feature \\(x_i\\) in class \\(C_k\\),\n",
        "- **count(C_k)** is the total count of all features for class \\(C_k\\),\n",
        "- **V** is the total number of possible distinct features (the vocabulary size).\n",
        "\n",
        "### 3. **Why Laplace Smoothing Helps**\n",
        "- **Avoiding Zero Probabilities:** The main benefit of Laplace smoothing is that it ensures every feature has a non-zero probability, even if it didn't appear in the training data for a particular class. This prevents the problem where a missing feature leads to a zero probability, which would invalidate the entire likelihood calculation.\n",
        "  \n",
        "- **Improved Generalization:** Laplace smoothing helps to generalize the model better by slightly \"smoothing\" the probability estimates. This can be especially helpful when working with small datasets or sparse data, where some feature-class combinations might not appear in the training set but could still appear in unseen test data.\n",
        "\n",
        "- **Better Performance with Rare Features:** In real-world datasets, some features may appear rarely. Laplace smoothing ensures that even rare features are given a small, non-zero probability, which is important for making reliable predictions, especially in applications like text classification (e.g., in spam filtering or sentiment analysis).\n",
        "\n",
        "### Example:\n",
        "Imagine you're building a Naïve Bayes classifier for spam email classification. You have the following:\n",
        "\n",
        "- **Training Data:** You have 1000 emails, 400 of which are spam and 600 are not spam.\n",
        "- **Vocabulary:** There are 500 unique words in your dataset.\n",
        "- Now, suppose the word \"win\" does not appear in any of the emails labeled as \"not spam.\" Without Laplace smoothing, the probability of \"win\" occurring given \"not spam\" would be zero.\n",
        "\n",
        "By applying Laplace smoothing, you would add 1 to the count of \"win\" in the \"not spam\" emails, which ensures that it gets a small probability instead of zero. This helps avoid misclassification of future emails with the word \"win.\"\n",
        "\n",
        "### Conclusion:\n",
        "Laplace smoothing is a simple but effective technique that ensures all feature probabilities are non-zero, leading to more robust Naïve Bayes models, especially when dealing with unseen data or sparse features. It is widely used in Naïve Bayes classification, particularly in text classification tasks where certain words may be absent in training data but appear in test data."
      ],
      "metadata": {
        "id": "bQmqqSf8jTu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Questions<"
      ],
      "metadata": {
        "id": "tY8cD86ljctD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.21 Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy:"
      ],
      "metadata": {
        "id": "ieuBgMGHj_jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Create an SVM classifier with a linear kernel\n",
        "svm = SVC(kernel='linear')\n",
        "\n",
        "# Train the classifier\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the SVM classifier: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goRsgYoYAMFa",
        "outputId": "814694bd-5ef0-4832-a68c-2e888c13e68b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SVM classifier: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.22  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies:"
      ],
      "metadata": {
        "id": "hws7BdfVkimd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Create SVM classifiers with linear and RBF kernels\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "\n",
        "# Train the classifiers\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifiers\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Accuracy of the SVM classifier with linear kernel: {accuracy_linear}\")\n",
        "print(f\"Accuracy of the SVM classifier with RBF kernel: {accuracy_rbf}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu0HfNrtAchR",
        "outputId": "2451e769-2e8b-405d-ec05-dde06c682356"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SVM classifier with linear kernel: 0.9814814814814815\n",
            "Accuracy of the SVM classifier with RBF kernel: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.23 Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE):"
      ],
      "metadata": {
        "id": "L73ja15Uk3rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the housing dataset (for demonstration purposes, we're using a random dataset)\n",
        "# In practice, you would load a real housing dataset, e.g., from sklearn.datasets or a CSV file\n",
        "# Example: housing_data = pd.read_csv('path_to_housing_dataset.csv')\n",
        "\n",
        "# For this demonstration, we generate some synthetic data\n",
        "# Let's assume the data has the following columns: 'Feature1', 'Feature2', ..., 'Price'\n",
        "# In a real case, you would replace this with actual data.\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 2)  # 100 samples, 2 features (e.g., square footage and number of rooms)\n",
        "y = X[:, 0] * 50 + X[:, 1] * 30 + np.random.randn(100) * 10  # Price target (some function of features with noise)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling - Standardize features to have zero mean and unit variance (important for SVR)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Initialize the SVR model\n",
        "svr = SVR(kernel='rbf', C=100, epsilon=0.1)\n",
        "\n",
        "# Train the SVR model\n",
        "svr.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_scaled = svr.predict(X_test_scaled)\n",
        "\n",
        "# Inverse transform the predictions to the original scale\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
        "y_test_original = scaler_y.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Evaluate the model using Mean Squared Error\n",
        "mse = mean_squared_error(y_test_original, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMlF7l_mBDf6",
        "outputId": "1d5de173-8457-43ab-be8f-ce8f960c821e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 109.10102274149173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.24 Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary."
      ],
      "metadata": {
        "id": "4r4LLcNzlXQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset (or any other 2-feature dataset for visualization)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # We only take the first two features for visualization\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Create an SVM classifier with a polynomial kernel\n",
        "svm = SVC(kernel='poly', degree=3, C=1.0) # Example parameters; adjust as needed\n",
        "\n",
        "# Train the classifier\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Create a meshgrid for plotting the decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Predict the class for each point in the meshgrid\n",
        "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary and the data points\n",
        "plt.contourf(xx, yy, Z, alpha=0.4)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.title('SVM with Polynomial Kernel (Decision Boundary)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "N7kKApMJBN9Y",
        "outputId": "865cba17-a5d8-4c50-f15a-0b636a780858"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.25 Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy:"
      ],
      "metadata": {
        "id": "671191L1BTkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Gaussian Naïve Bayes classifier: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3NmcGF1C8U_",
        "outputId": "9d5e5ea7-be55-4783-f24c-f8dcea74d457"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Gaussian Naïve Bayes classifier: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.26 Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset."
      ],
      "metadata": {
        "id": "9mtx4SWqBoPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "# Create a pipeline with TF-IDF vectorizer and Multinomial Naive Bayes classifier\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "# Train the model\n",
        "model.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predicted = model.predict(newsgroups_test.data)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(newsgroups_test.target, predicted)\n",
        "print(f\"Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuMsgslYDFF5",
        "outputId": "615922e2-2080-4975-d459-69e0c4d60d75"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7738980350504514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.27 Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually."
      ],
      "metadata": {
        "id": "ezVJ0x_HB0Wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# C values to test\n",
        "C_values = [0.1, 1, 10]\n",
        "\n",
        "# Create subplots for each C value\n",
        "fig, axes = plt.subplots(1, len(C_values), figsize=(15, 5))\n",
        "\n",
        "# Loop through different C values\n",
        "for i, C in enumerate(C_values):\n",
        "  # Create and train the SVM classifier\n",
        "  svm = SVC(kernel='linear', C=C)\n",
        "  svm.fit(X_train, y_train)\n",
        "\n",
        "  # Plot decision boundary\n",
        "  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
        "  Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  axes[i].contourf(xx, yy, Z, alpha=0.4)\n",
        "\n",
        "  # Plot data points\n",
        "  axes[i].scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k')\n",
        "  axes[i].set_title(f'C = {C}')\n",
        "  axes[i].set_xlabel('Sepal length')\n",
        "  axes[i].set_ylabel('Sepal width')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "CbDH5Kv4DZph",
        "outputId": "6ffac597-1560-4fd2-bcdd-faae8c467639"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.28 Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features."
      ],
      "metadata": {
        "id": "zwu4q6lrCNnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic binary data\n",
        "np.random.seed(0)\n",
        "X = np.random.randint(0, 2, size=(100, 5))  # 100 samples, 5 binary features\n",
        "y = np.random.randint(0, 2, size=100)      # Binary target variable\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Bernoulli Naïve Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Bernoulli Naïve Bayes classifier: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES4UTa5ODkgV",
        "outputId": "40bd7e46-5fc9-4cc8-fe57-182f7aa65038"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Bernoulli Naïve Bayes classifier: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.29 Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data."
      ],
      "metadata": {
        "id": "ZhSPvb20Cc7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train and evaluate the model on unscaled data\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_classifier.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply Feature Scaling (Standardization) to the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate the model on scaled data\n",
        "svm_classifier.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_classifier.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy with unscaled data: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy with scaled data: {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "1UVqYCGhv-LB",
        "outputId": "de9f3d99-8cc0-40cb-92d0-9edadce0192b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with unscaled data: 0.9561\n",
            "Accuracy with scaled data: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.30 Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing."
      ],
      "metadata": {
        "id": "KBSsqJJjCmcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes without Laplace smoothing\n",
        "gnb_no_smoothing = GaussianNB(var_smoothing=0)  # var_smoothing=0 effectively disables Laplace smoothing\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "print(f\"Accuracy without Laplace smoothing: {accuracy_no_smoothing}\")\n",
        "\n",
        "# Train Gaussian Naïve Bayes with Laplace smoothing (default behavior)\n",
        "gnb_with_smoothing = GaussianNB()\n",
        "gnb_with_smoothing.fit(X_train, y_train)\n",
        "y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
        "print(f\"Accuracy with Laplace smoothing: {accuracy_with_smoothing}\")\n",
        "\n",
        "# Compare predictions\n",
        "print(\"\\nComparison of predictions:\")\n",
        "comparison = np.array([y_pred_no_smoothing, y_pred_with_smoothing, y_test])\n",
        "comparison.T\n"
      ],
      "metadata": {
        "id": "JSKEPklovD2t",
        "outputId": "cc780d07-37b2-4387-a0c8-83a0afd1cc00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Laplace smoothing: 0.9777777777777777\n",
            "Accuracy with Laplace smoothing: 0.9777777777777777\n",
            "\n",
            "Comparison of predictions:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1],\n",
              "       [0, 0, 0],\n",
              "       [2, 2, 2],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 0, 0],\n",
              "       [1, 1, 1],\n",
              "       [2, 2, 2],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [2, 2, 2],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [2, 2, 1],\n",
              "       [2, 2, 2],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [2, 2, 2],\n",
              "       [0, 0, 0],\n",
              "       [2, 2, 2],\n",
              "       [0, 0, 0],\n",
              "       [2, 2, 2],\n",
              "       [2, 2, 2],\n",
              "       [2, 2, 2],\n",
              "       [2, 2, 2],\n",
              "       [2, 2, 2],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [1, 1, 1],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [2, 2, 2],\n",
              "       [1, 1, 1],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0],\n",
              "       [2, 2, 2],\n",
              "       [1, 1, 1],\n",
              "       [1, 1, 1],\n",
              "       [0, 0, 0],\n",
              "       [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.31 Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)."
      ],
      "metadata": {
        "id": "EJfBHciCED6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "# Create an SVM classifier\n",
        "svc = SVC()\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid = GridSearchCV(svc, param_grid, refit=True, verbose=3)\n",
        "\n",
        "# Fit the GridSearchCV object to the training data\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(f\"Best parameters: {grid.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid.best_score_}\")\n",
        "\n",
        "# Make predictions on the test set using the best estimator\n",
        "grid_predictions = grid.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the tuned classifier\n",
        "accuracy = accuracy_score(y_test, grid_predictions)\n",
        "print(f\"Accuracy of the tuned SVM classifier: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l5T_RLmEYqR",
        "outputId": "b0aea075-8771-4897-b8d8-f1290a97c954"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 3/5] END ........C=0.1, gamma=1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END .......C=0.1, gamma=1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END .......C=0.1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END .......C=0.1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .......C=0.1, gamma=1, kernel=poly;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END .......C=0.1, gamma=1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ....C=0.1, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ....C=0.1, gamma=1, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ....C=0.1, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ....C=0.1, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ....C=0.1, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.810 total time=   0.0s\n",
            "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.857 total time=   0.0s\n",
            "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.857 total time=   0.0s\n",
            "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 1/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 5/5] END .....C=0.1, gamma=0.1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ..C=0.1, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ..C=0.1, gamma=0.1, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ..C=0.1, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ..C=0.1, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ..C=0.1, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.667 total time=   0.0s\n",
            "[CV 2/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.667 total time=   0.0s\n",
            "[CV 3/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
            "[CV 4/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
            "[CV 5/5] END .....C=0.1, gamma=0.01, kernel=rbf;, score=0.714 total time=   0.0s\n",
            "[CV 1/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.476 total time=   0.0s\n",
            "[CV 2/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.619 total time=   0.0s\n",
            "[CV 3/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.667 total time=   0.0s\n",
            "[CV 4/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.714 total time=   0.0s\n",
            "[CV 5/5] END ....C=0.1, gamma=0.01, kernel=poly;, score=0.667 total time=   0.0s\n",
            "[CV 1/5] END .C=0.1, gamma=0.01, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END .C=0.1, gamma=0.01, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END .C=0.1, gamma=0.01, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END .C=0.1, gamma=0.01, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END .C=0.1, gamma=0.01, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ....C=0.1, gamma=0.001, kernel=rbf;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ...C=0.1, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END C=0.1, gamma=0.001, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END C=0.1, gamma=0.001, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END C=0.1, gamma=0.001, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END C=0.1, gamma=0.001, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END C=0.1, gamma=0.001, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END .........C=1, gamma=1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END .........C=1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END .........C=1, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .........C=1, gamma=1, kernel=poly;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END .........C=1, gamma=1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ......C=1, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ......C=1, gamma=1, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ......C=1, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ......C=1, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ......C=1, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END .......C=1, gamma=0.1, kernel=poly;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END .......C=1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END .......C=1, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .......C=1, gamma=0.1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 5/5] END .......C=1, gamma=0.1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ....C=1, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ....C=1, gamma=0.1, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ....C=1, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ....C=1, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ....C=1, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 3/5] END .......C=1, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END .......C=1, gamma=0.01, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 1/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.714 total time=   0.0s\n",
            "[CV 2/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.810 total time=   0.0s\n",
            "[CV 3/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.905 total time=   0.0s\n",
            "[CV 4/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END ......C=1, gamma=0.01, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ...C=1, gamma=0.01, kernel=sigmoid;, score=0.667 total time=   0.0s\n",
            "[CV 2/5] END ...C=1, gamma=0.01, kernel=sigmoid;, score=0.667 total time=   0.0s\n",
            "[CV 3/5] END ...C=1, gamma=0.01, kernel=sigmoid;, score=0.714 total time=   0.0s\n",
            "[CV 4/5] END ...C=1, gamma=0.01, kernel=sigmoid;, score=0.714 total time=   0.0s\n",
            "[CV 5/5] END ...C=1, gamma=0.01, kernel=sigmoid;, score=0.714 total time=   0.0s\n",
            "[CV 1/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.667 total time=   0.0s\n",
            "[CV 2/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.667 total time=   0.0s\n",
            "[CV 3/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.714 total time=   0.0s\n",
            "[CV 4/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.714 total time=   0.0s\n",
            "[CV 5/5] END ......C=1, gamma=0.001, kernel=rbf;, score=0.714 total time=   0.0s\n",
            "[CV 1/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END .....C=1, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END ..C=1, gamma=0.001, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ..C=1, gamma=0.001, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ..C=1, gamma=0.001, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ..C=1, gamma=0.001, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ..C=1, gamma=0.001, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.857 total time=   0.0s\n",
            "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ........C=10, gamma=1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END ........C=10, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END ........C=10, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ........C=10, gamma=1, kernel=poly;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END ........C=10, gamma=1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END .....C=10, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END .....C=10, gamma=1, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END .....C=10, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END .....C=10, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END .....C=10, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ......C=10, gamma=0.1, kernel=poly;, score=0.905 total time=   0.0s\n",
            "[CV 2/5] END ......C=10, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END ......C=10, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ......C=10, gamma=0.1, kernel=poly;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END ......C=10, gamma=0.1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ...C=10, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ...C=10, gamma=0.1, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ...C=10, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ...C=10, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ...C=10, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END ......C=10, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END ......C=10, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END ......C=10, gamma=0.01, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END .....C=10, gamma=0.01, kernel=poly;, score=0.905 total time=   0.0s\n",
            "[CV 2/5] END .....C=10, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END .....C=10, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .....C=10, gamma=0.01, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 5/5] END .....C=10, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 1/5] END ..C=10, gamma=0.01, kernel=sigmoid;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END ..C=10, gamma=0.01, kernel=sigmoid;, score=0.857 total time=   0.0s\n",
            "[CV 3/5] END ..C=10, gamma=0.01, kernel=sigmoid;, score=0.952 total time=   0.0s\n",
            "[CV 4/5] END ..C=10, gamma=0.01, kernel=sigmoid;, score=0.857 total time=   0.0s\n",
            "[CV 5/5] END ..C=10, gamma=0.01, kernel=sigmoid;, score=0.857 total time=   0.0s\n",
            "[CV 1/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 3/5] END .....C=10, gamma=0.001, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END .....C=10, gamma=0.001, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 1/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ....C=10, gamma=0.001, kernel=poly;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END .C=10, gamma=0.001, kernel=sigmoid;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END .C=10, gamma=0.001, kernel=sigmoid;, score=0.762 total time=   0.0s\n",
            "[CV 3/5] END .C=10, gamma=0.001, kernel=sigmoid;, score=0.857 total time=   0.0s\n",
            "[CV 4/5] END .C=10, gamma=0.001, kernel=sigmoid;, score=0.762 total time=   0.0s\n",
            "[CV 5/5] END .C=10, gamma=0.001, kernel=sigmoid;, score=0.857 total time=   0.0s\n",
            "[CV 1/5] END ........C=100, gamma=1, kernel=rbf;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END ........C=100, gamma=1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ........C=100, gamma=1, kernel=rbf;, score=0.857 total time=   0.0s\n",
            "[CV 5/5] END ........C=100, gamma=1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END .......C=100, gamma=1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END .......C=100, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END .......C=100, gamma=1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .......C=100, gamma=1, kernel=poly;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END .......C=100, gamma=1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ....C=100, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ....C=100, gamma=1, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ....C=100, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ....C=100, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ....C=100, gamma=1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END ......C=100, gamma=0.1, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.857 total time=   0.0s\n",
            "[CV 5/5] END ......C=100, gamma=0.1, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END .....C=100, gamma=0.1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END .....C=100, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END .....C=100, gamma=0.1, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .....C=100, gamma=0.1, kernel=poly;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END .....C=100, gamma=0.1, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ..C=100, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 2/5] END ..C=100, gamma=0.1, kernel=sigmoid;, score=0.333 total time=   0.0s\n",
            "[CV 3/5] END ..C=100, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 4/5] END ..C=100, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 5/5] END ..C=100, gamma=0.1, kernel=sigmoid;, score=0.381 total time=   0.0s\n",
            "[CV 1/5] END .....C=100, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 2/5] END .....C=100, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END .....C=100, gamma=0.01, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 5/5] END .....C=100, gamma=0.01, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ....C=100, gamma=0.01, kernel=poly;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END ....C=100, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END ....C=100, gamma=0.01, kernel=poly;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ....C=100, gamma=0.01, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 5/5] END ....C=100, gamma=0.01, kernel=poly;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END .C=100, gamma=0.01, kernel=sigmoid;, score=0.857 total time=   0.0s\n",
            "[CV 2/5] END .C=100, gamma=0.01, kernel=sigmoid;, score=0.810 total time=   0.0s\n",
            "[CV 3/5] END .C=100, gamma=0.01, kernel=sigmoid;, score=0.905 total time=   0.0s\n",
            "[CV 4/5] END .C=100, gamma=0.01, kernel=sigmoid;, score=0.714 total time=   0.0s\n",
            "[CV 5/5] END .C=100, gamma=0.01, kernel=sigmoid;, score=0.810 total time=   0.0s\n",
            "[CV 1/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 2/5] END ....C=100, gamma=0.001, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END ....C=100, gamma=0.001, kernel=rbf;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END ....C=100, gamma=0.001, kernel=rbf;, score=0.952 total time=   0.0s\n",
            "[CV 1/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.476 total time=   0.0s\n",
            "[CV 2/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.619 total time=   0.0s\n",
            "[CV 3/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.667 total time=   0.0s\n",
            "[CV 4/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.714 total time=   0.0s\n",
            "[CV 5/5] END ...C=100, gamma=0.001, kernel=poly;, score=0.667 total time=   0.0s\n",
            "[CV 1/5] END C=100, gamma=0.001, kernel=sigmoid;, score=0.905 total time=   0.0s\n",
            "[CV 2/5] END C=100, gamma=0.001, kernel=sigmoid;, score=1.000 total time=   0.0s\n",
            "[CV 3/5] END C=100, gamma=0.001, kernel=sigmoid;, score=1.000 total time=   0.0s\n",
            "[CV 4/5] END C=100, gamma=0.001, kernel=sigmoid;, score=0.905 total time=   0.0s\n",
            "[CV 5/5] END C=100, gamma=0.001, kernel=sigmoid;, score=1.000 total time=   0.0s\n",
            "Best parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
            "Best cross-validation score: 0.980952380952381\n",
            "Accuracy of the tuned SVM classifier: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.32 Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy."
      ],
      "metadata": {
        "id": "3c5v_PhKEfGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "# Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2,\n",
        "                           n_redundant=0, n_repeated=0, n_classes=2,\n",
        "                           n_clusters_per_class=1, weights=[0.9, 0.1],\n",
        "                           flip_y=0, random_state=42)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM without class weights\n",
        "svm_no_weights = SVC()\n",
        "svm_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = svm_no_weights.predict(X_test)\n",
        "accuracy_no_weights = accuracy_score(y_test, y_pred_no_weights)\n",
        "print(f\"Accuracy (without class weights): {accuracy_no_weights}\")\n",
        "\n",
        "# Calculate class weights (inverse of class frequencies)\n",
        "class_weights = {0: 1, 1: 10}  # Adjust weights as needed, experiment with different values\n",
        "\n",
        "# Train an SVM with class weights\n",
        "svm_with_weights = SVC(class_weight=class_weights)\n",
        "svm_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = svm_with_weights.predict(X_test)\n",
        "accuracy_with_weights = accuracy_score(y_test, y_pred_with_weights)\n",
        "print(f\"Accuracy (with class weights): {accuracy_with_weights}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuhlQrunHzyS",
        "outputId": "d47266a7-26bd-4f48-b666-254dde3124a7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (without class weights): 0.975\n",
            "Accuracy (with class weights): 0.985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.33 Write a Python program to implement a Naïve Bayes classifier for spam detection using email data."
      ],
      "metadata": {
        "id": "Y_dIggbqEw6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Sample email data (replace with your actual dataset)\n",
        "data = {\n",
        "    'email': [\n",
        "        'Free Viagra now!',\n",
        "        'Hi, how are you?',\n",
        "        'Congratulations! You won a prize!',\n",
        "        'Meeting tomorrow at 10am.',\n",
        "        'Get rich quick!',\n",
        "        'Check out our latest product!',\n",
        "        'Your order has been shipped.',\n",
        "    ],\n",
        "    'spam': [1, 0, 1, 0, 1, 1, 0]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['email'], df['spam'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Multinomial Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = classifier.predict(X_test_vec)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "M9uJSaivnqPw",
        "outputId": "4ebcdafe-08d8-4eef-d496-327c6b8f9aa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.50      1.00      0.67         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.34 Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and Compare their accuracy."
      ],
      "metadata": {
        "id": "7o3okrO2E5dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (Iris dataset as an example)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear')  # Using a linear kernel for simplicity\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Train the Naïve Bayes Classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict using both classifiers\n",
        "svm_pred = svm_classifier.predict(X_test)\n",
        "nb_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of both classifiers\n",
        "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
        "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
        "\n",
        "# Output the results\n",
        "print(f\"SVM Classifier Accuracy: {svm_accuracy:.4f}\")\n",
        "print(f\"Naïve Bayes Classifier Accuracy: {nb_accuracy:.4f}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "if svm_accuracy > nb_accuracy:\n",
        "    print(\"SVM Classifier performed better.\")\n",
        "elif nb_accuracy > svm_accuracy:\n",
        "    print(\"Naïve Bayes Classifier performed better.\")\n",
        "else:\n",
        "    print(\"Both classifiers performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb-87x-gJAxn",
        "outputId": "094c4bed-c644-4bcc-f170-9256f25b7bd5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier Accuracy: 1.0000\n",
            "Naïve Bayes Classifier Accuracy: 0.9778\n",
            "SVM Classifier performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.35 Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare Results.\n"
      ],
      "metadata": {
        "id": "WjLekY2WFGNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Naïve Bayes without feature selection ---\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy_no_selection = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy (without feature selection): {accuracy_no_selection}\")\n",
        "\n",
        "# --- Naïve Bayes with feature selection ---\n",
        "# Select the top 2 features based on ANOVA F-value\n",
        "selector = SelectKBest(f_classif, k=2)  # Experiment with different values of k\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "gnb_selected = GaussianNB()\n",
        "gnb_selected.fit(X_train_selected, y_train)\n",
        "y_pred_selected = gnb_selected.predict(X_test_selected)\n",
        "accuracy_with_selection = accuracy_score(y_test, y_pred_selected)\n",
        "print(f\"Accuracy (with feature selection): {accuracy_with_selection}\")\n",
        "\n",
        "# Compare results\n",
        "if accuracy_no_selection > accuracy_with_selection:\n",
        "    print(\"Naïve Bayes without feature selection performed better.\")\n",
        "elif accuracy_with_selection > accuracy_no_selection:\n",
        "    print(\"Naïve Bayes with feature selection performed better.\")\n",
        "else:\n",
        "    print(\"Both performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDHN192YJPI0",
        "outputId": "51761971-d5b7-4005-bf2d-228cea627ba9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (without feature selection): 0.9777777777777777\n",
            "Accuracy (with feature selection): 1.0\n",
            "Naïve Bayes with feature selection performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.36  Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy."
      ],
      "metadata": {
        "id": "vl45ic3TFU0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import datasets\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# One-vs-Rest (OvR)\n",
        "ovr_classifier = OneVsRestClassifier(SVC(kernel='linear')) # You can change the kernel if needed\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "ovr_predictions = ovr_classifier.predict(X_test)\n",
        "ovr_accuracy = accuracy_score(y_test, ovr_predictions)\n",
        "print(f\"One-vs-Rest Accuracy: {ovr_accuracy}\")\n",
        "\n",
        "# One-vs-One (OvO)\n",
        "ovo_classifier = OneVsOneClassifier(SVC(kernel='linear')) # You can change the kernel if needed\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "ovo_predictions = ovo_classifier.predict(X_test)\n",
        "ovo_accuracy = accuracy_score(y_test, ovo_predictions)\n",
        "print(f\"One-vs-One Accuracy: {ovo_accuracy}\")\n",
        "\n",
        "# Compare accuracies\n",
        "if ovr_accuracy > ovo_accuracy:\n",
        "    print(\"One-vs-Rest performed better.\")\n",
        "elif ovo_accuracy > ovr_accuracy:\n",
        "    print(\"One-vs-One performed better.\")\n",
        "else:\n",
        "    print(\"Both performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPzpJDRAJbSZ",
        "outputId": "db14f67c-17e1-428a-b6af-4aef79774ea6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-Rest Accuracy: 0.9814814814814815\n",
            "One-vs-One Accuracy: 0.9814814814814815\n",
            "Both performed equally well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.37 Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy."
      ],
      "metadata": {
        "id": "BMQ4AV40Fiyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize SVM classifiers with different kernels\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_poly = SVC(kernel='poly', degree=3)  # Polynomial kernel with degree 3\n",
        "svm_rbf = SVC(kernel='rbf')  # Radial Basis Function kernel\n",
        "\n",
        "# Train the classifiers\n",
        "svm_linear.fit(X_train, y_train)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_linear = svm\n"
      ],
      "metadata": {
        "id": "D5kbZB1ggynY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.38 Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy."
      ],
      "metadata": {
        "id": "dIjUwSyOFt_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Initialize Stratified K-Fold Cross-Validation (5 folds)\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize SVM classifier with RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# List to store the accuracy of each fold\n",
        "accuracies = []\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    # Split data into training and testing based on the current fold\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the SVM model on the current fold\n"
      ],
      "metadata": {
        "id": "X5pwiupehcuS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.39 Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance."
      ],
      "metadata": {
        "id": "6sy9RyXHF4bR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define prior probabilities to test\n",
        "prior_probabilities = [\n",
        "    None,  # Default priors (calculated from the training data)\n",
        "    [0.2, 0.8],  # Example: Prior probability of class 0 is 0.2, class 1 is 0.8\n",
        "    [0.5, 0.5], # Example: equal prior probabilities\n",
        "    [0.8, 0.2]\n",
        "]\n",
        "\n",
        "for priors in prior_probabilities:\n",
        "    # Initialize and train the Gaussian Naïve Bayes classifier with different prior probabilities\n",
        "    gnb = GaussianNB(priors=priors)\n",
        "    gnb.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = gnb.predict(X_test)\n",
        "\n",
        "    # Evaluate the accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    if priors is None:\n",
        "        print(f\"Accuracy with default priors: {accuracy}\")\n",
        "    else:\n",
        "        print(f\"Accuracy with priors {priors}: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "T8H3Q2Gbhu28",
        "outputId": "4a76678e-b22b-4050-9b34-4232657f3b37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with default priors: 0.9736842105263158\n",
            "Accuracy with priors [0.2, 0.8]: 0.9649122807017544\n",
            "Accuracy with priors [0.5, 0.5]: 0.9736842105263158\n",
            "Accuracy with priors [0.8, 0.2]: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.40 Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy."
      ],
      "metadata": {
        "id": "ksdvaAbPGAsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- SVM without RFE ---\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred = svm.predict(X_test)\n",
        "accuracy_no_rfe = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy (without RFE): {accuracy_no_rfe}\")\n",
        "\n",
        "# --- SVM with RFE ---\n",
        "estimator = SVC(kernel='linear') # You can experiment with different kernels\n",
        "selector = RFE(estimator, n_features_to_select=10, step=1) # Select top 10 features\n",
        "X_train_rfe = selector.fit_transform(X_train, y_train)\n",
        "X_test_rfe = selector.transform(X_test)\n",
        "\n",
        "svm_rfe = SVC()\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "accuracy_with_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "print(f\"Accuracy (with RFE): {accuracy_with_rfe}\")\n",
        "\n",
        "# Compare results\n",
        "if accuracy_no_rfe > accuracy_with_rfe:\n",
        "    print(\"SVM without RFE performed better.\")\n",
        "elif accuracy_with_rfe > accuracy_no_rfe:\n",
        "    print(\"SVM with RFE performed better.\")\n",
        "else:\n",
        "    print(\"Both performed equally well.\")\n"
      ],
      "metadata": {
        "id": "l81khUaOh7D2",
        "outputId": "8985422e-7ccd-4f8a-c12f-3909dad019ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (without RFE): 0.9473684210526315\n",
            "Accuracy (with RFE): 0.9473684210526315\n",
            "Both performed equally well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.41 Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy."
      ],
      "metadata": {
        "id": "9xOlwTpGGR0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "# Train the SVM Classifier (example using the Iris dataset)\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "svm_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(y_test, svm_pred, average='weighted') # Use 'macro', 'micro' or 'weighted' as needed\n",
        "recall = recall_score(y_test, svm_pred, average='weighted')\n",
        "f1 = f1_score(y_test, svm_pred, average='weighted')\n",
        "\n",
        "print(f\"SVM Classifier Precision: {precision:.4f}\")\n",
        "print(f\"SVM Classifier Recall: {recall:.4f}\")\n",
        "print(f\"SVM Classifier F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "nGqZq0CJiIcJ",
        "outputId": "d538727f-80d5-40de-8f3e-73da6b78482b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Classifier Precision: 0.9569\n",
            "SVM Classifier Recall: 0.9561\n",
            "SVM Classifier F1-Score: 0.9558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.42 Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss).\n"
      ],
      "metadata": {
        "id": "4AL0qILOGeZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions on the test set\n",
        "y_pred_proba = gnb.predict_proba(X_test)\n",
        "\n",
        "# Calculate the log loss (cross-entropy loss)\n",
        "logloss = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"Log Loss (Cross-Entropy Loss): {logloss}\")\n"
      ],
      "metadata": {
        "id": "1V_cwesEiTM-",
        "outputId": "9f3bea49-440d-4a9d-eab6-ab48c588884d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Loss (Cross-Entropy Loss): 0.20373427152141899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.43 Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn.\n"
      ],
      "metadata": {
        "id": "qbGajmtlGpgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the SVM model\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot the confusion matrix using Seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
        "plt.title('Confusion Matrix for SVM Classifier')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vnIoLfBfm8bq",
        "outputId": "23e62b6c-911a-4eac-a3ec-28988641ab60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.44 Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE."
      ],
      "metadata": {
        "id": "6fOReHWrG2Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM Regressor (SVR) with an RBF kernel\n",
        "svr = SVR(kernel='rbf')\n",
        "\n",
        "# Train the SVR model on the training data\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Compute the Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print the Mean Absolute Error\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
      ],
      "metadata": {
        "id": "akYeOQqvjd14",
        "outputId": "b6936df6-74d8-43c1-a021-e735fdffa547",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 0.8600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.45  Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score."
      ],
      "metadata": {
        "id": "dxGdjxzuHGNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load the Breast Cancer dataset (or any binary classification dataset)\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make probability predictions on the test set\n",
        "y_pred_proba = gnb.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"ROC-AUC Score: {roc_auc}\")\n"
      ],
      "metadata": {
        "id": "_Xd6Mzqak_1X",
        "outputId": "26d14e73-9689-468a-9557-a983f984717f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9983622666229938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###QUES.46 Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."
      ],
      "metadata": {
        "id": "Xq6hG4HfHZZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your dataset (replace with your actual data)\n",
        "# Example using the breast cancer dataset:\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train an SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', probability=True) # probability=True is crucial for predict_proba\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_scores = svm_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision, recall, and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Calculate the AUC (Area Under the Curve)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Plot the Precision-Recall Curve\n",
        "plt.plot(recall, precision, label=f'Precision-Recall Curve (AUC = {pr_auc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QoZbf4Exw1qW",
        "outputId": "2cc7b271-5526-47d6-e43b-bcfa914a4228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#THE END"
      ],
      "metadata": {
        "id": "rRRWUZF2xBxH"
      }
    }
  ]
}